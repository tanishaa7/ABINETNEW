global:
  name: pretrain-language-model
  phase: train
  stage: pretrain-language
  workdir: workdir
  seed: ~
 
dataset:
  train: {
    #roots: ['data/WikiText-103.csv'],
    roots: ['data/wiki_bg_text.csv'],
    #roots: ['/home2/tanisha/CUTE80'],
    # batch_size: 4096
    batch_size : 2048
  }
  test: {
    #roots: ['data/WikiText-103_eval_d1.csv'],
    roots: ['data/wiki_bg_text_eval.csv'],
    # batch_size: 4096
    batch_size: 2048
  }

training:
  # epochs: 80
  epochs: 10
  show_iters: 50
  # eval_iters: 6000
  # save_iters: 3000
  eval_iters: 6000
  save_iters: 1000

optimizer:
  type: Adam
  true_wd: False
  wd: 0.0
  bn_wd: False
  clip_grad: 20
  lr: 0.0001
  args: {
    betas: !!python/tuple [0.9, 0.999], # for default Adam 
  }
  scheduler: {
    periods: [70, 10],
    gamma: 0.1,
  }

model:
  name: 'modules.model_language.BCNLanguage'
  language: {
    num_layers: 4,
    loss_weight: 1.,
    use_self_attn: False
  }
