ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2522,  smooth loss = 2.4595
epoch 0 iter 100: loss = 2.0135,  smooth loss = 2.2076
epoch 0 iter 150: loss = 1.8111,  smooth loss = 1.9843
epoch 0 iter 200: loss = 1.6503,  smooth loss = 1.8059
epoch 0 iter 250: loss = 1.5992,  smooth loss = 1.6802
epoch 0 iter 300: loss = 1.5137,  smooth loss = 1.5875
epoch 0 iter 350: loss = 1.4345,  smooth loss = 1.5102
epoch 0 iter 400: loss = 1.3648,  smooth loss = 1.4500
epoch 0 iter 450: loss = 1.3240,  smooth loss = 1.3972
epoch 0 iter 500: loss = 1.3065,  smooth loss = 1.3460
epoch 0 iter 550: loss = 1.2239,  smooth loss = 1.3016
epoch 0 iter 600: loss = 1.2577,  smooth loss = 1.2581
epoch 0 iter 650: loss = 1.1766,  smooth loss = 1.2205
epoch 0 iter 700: loss = 1.1032,  smooth loss = 1.1850
epoch 0 iter 750: loss = 1.1415,  smooth loss = 1.1574
epoch 0 iter 800: loss = 1.0829,  smooth loss = 1.1237
epoch 0 iter 850: loss = 1.0782,  smooth loss = 1.0946
epoch 0 iter 900: loss = 1.0070,  smooth loss = 1.0675
epoch 0 iter 950: loss = 0.9976,  smooth loss = 1.0414
epoch 0 iter 1000: loss = 1.0287,  smooth loss = 1.0181
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9878,  smooth loss = 0.9926
epoch 0 iter 1100: loss = 0.9713,  smooth loss = 0.9685
epoch 0 iter 1150: loss = 0.9130,  smooth loss = 0.9476
epoch 0 iter 1200: loss = 0.9028,  smooth loss = 0.9278
epoch 0 iter 1250: loss = 0.8533,  smooth loss = 0.9063
epoch 0 iter 1300: loss = 0.8985,  smooth loss = 0.8869
epoch 0 iter 1350: loss = 0.8552,  smooth loss = 0.8701
epoch 0 iter 1400: loss = 0.8303,  smooth loss = 0.8548
epoch 0 iter 1450: loss = 0.8165,  smooth loss = 0.8429
epoch 0 iter 1500: loss = 0.7904,  smooth loss = 0.8241
epoch 0 iter 1550: loss = 0.7871,  smooth loss = 0.8095
epoch 0 iter 1600: loss = 0.7948,  smooth loss = 0.7974
epoch 0 iter 1650: loss = 0.7646,  smooth loss = 0.7847
epoch 0 iter 1700: loss = 0.7469,  smooth loss = 0.7701
epoch 0 iter 1750: loss = 0.7219,  smooth loss = 0.7609
epoch 0 iter 1800: loss = 0.7055,  smooth loss = 0.7510
epoch 0 iter 1850: loss = 0.7102,  smooth loss = 0.7355
epoch 0 iter 1900: loss = 0.7204,  smooth loss = 0.7262
epoch 0 iter 1950: loss = 0.6978,  smooth loss = 0.7143
epoch 0 iter 2000: loss = 0.7341,  smooth loss = 0.7067
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.7147,  smooth loss = 0.7002
epoch 0 iter 2100: loss = 0.6990,  smooth loss = 0.6885
epoch 0 iter 2150: loss = 0.6546,  smooth loss = 0.6782
epoch 0 iter 2200: loss = 0.6726,  smooth loss = 0.6741
epoch 0 iter 2250: loss = 0.6962,  smooth loss = 0.6642
epoch 0 iter 2300: loss = 0.6544,  smooth loss = 0.6542
epoch 0 iter 2350: loss = 0.6422,  smooth loss = 0.6477
epoch 0 iter 2400: loss = 0.6419,  smooth loss = 0.6435
epoch 0 iter 2450: loss = 0.6424,  smooth loss = 0.6369
epoch 0 iter 2500: loss = 0.6144,  smooth loss = 0.6291
epoch 0 iter 2550: loss = 0.6025,  smooth loss = 0.6221
epoch 0 iter 2600: loss = 0.6079,  smooth loss = 0.6192
epoch 0 iter 2650: loss = 0.5770,  smooth loss = 0.6107
epoch 0 iter 2700: loss = 0.5933,  smooth loss = 0.6044
epoch 0 iter 2750: loss = 0.5753,  smooth loss = 0.5959
epoch 0 iter 2800: loss = 0.6068,  smooth loss = 0.5920
epoch 0 iter 2850: loss = 0.5612,  smooth loss = 0.5845
epoch 0 iter 2900: loss = 0.5840,  smooth loss = 0.5790
epoch 0 iter 2950: loss = 0.5802,  smooth loss = 0.5740
epoch 0 iter 3000: loss = 0.5986,  smooth loss = 0.5718
average data time = 0.0071s, average running time = 0.8325s
epoch 0 iter 3000: eval loss = 1.8010,  ccr = 0.8361,  cwr = 0.4873,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4873.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5894,  smooth loss = 0.5685
epoch 0 iter 3100: loss = 0.5597,  smooth loss = 0.5645
epoch 0 iter 3150: loss = 0.5397,  smooth loss = 0.5579
epoch 0 iter 3200: loss = 0.5561,  smooth loss = 0.5550
epoch 0 iter 3250: loss = 0.5057,  smooth loss = 0.5515
epoch 0 iter 3300: loss = 0.5403,  smooth loss = 0.5433
epoch 0 iter 3350: loss = 0.5306,  smooth loss = 0.5405
epoch 0 iter 3400: loss = 0.5448,  smooth loss = 0.5417
epoch 0 iter 3450: loss = 0.5386,  smooth loss = 0.5351
epoch 0 iter 3500: loss = 0.5130,  smooth loss = 0.5318
epoch 0 iter 3550: loss = 0.5311,  smooth loss = 0.5270
epoch 0 iter 3600: loss = 0.5544,  smooth loss = 0.5252
epoch 0 iter 3650: loss = 0.5242,  smooth loss = 0.5232
epoch 0 iter 3700: loss = 0.5299,  smooth loss = 0.5173
epoch 0 iter 3750: loss = 0.5047,  smooth loss = 0.5120
epoch 0 iter 3800: loss = 0.5493,  smooth loss = 0.5102
epoch 0 iter 3850: loss = 0.4823,  smooth loss = 0.5063
epoch 0 iter 3900: loss = 0.5062,  smooth loss = 0.5034
epoch 0 iter 3950: loss = 0.4952,  smooth loss = 0.5024
epoch 0 iter 4000: loss = 0.4849,  smooth loss = 0.5014
Save model pretrain-language-model_0_4000
epoch 0 iter 4050: loss = 0.4979,  smooth loss = 0.4941
epoch 0 iter 4100: loss = 0.4891,  smooth loss = 0.4915
epoch 0 iter 4150: loss = 0.5080,  smooth loss = 0.4906
epoch 0 iter 4200: loss = 0.4887,  smooth loss = 0.4869
epoch 0 iter 4250: loss = 0.4815,  smooth loss = 0.4845
epoch 0 iter 4300: loss = 0.4923,  smooth loss = 0.4810
epoch 0 iter 4350: loss = 0.5078,  smooth loss = 0.4802
epoch 0 iter 4400: loss = 0.4862,  smooth loss = 0.4773
epoch 0 iter 4450: loss = 0.4827,  smooth loss = 0.4739
epoch 0 iter 4500: loss = 0.4834,  smooth loss = 0.4709
epoch 0 iter 4550: loss = 0.4791,  smooth loss = 0.4704
epoch 0 iter 4600: loss = 0.4498,  smooth loss = 0.4676
epoch 0 iter 4650: loss = 0.4517,  smooth loss = 0.4659
epoch 0 iter 4700: loss = 0.4792,  smooth loss = 0.4633
epoch 0 iter 4750: loss = 0.4452,  smooth loss = 0.4614
epoch 0 iter 4800: loss = 0.4372,  smooth loss = 0.4599
epoch 0 iter 4850: loss = 0.4577,  smooth loss = 0.4614
epoch 0 iter 4900: loss = 0.4841,  smooth loss = 0.4552
epoch 0 iter 4950: loss = 0.4747,  smooth loss = 0.4522
epoch 0 iter 5000: loss = 0.4397,  smooth loss = 0.4506
Save model pretrain-language-model_0_5000
epoch 0 iter 5050: loss = 0.4138,  smooth loss = 0.4488
epoch 0 iter 5100: loss = 0.4465,  smooth loss = 0.4492
epoch 0 iter 5150: loss = 0.4397,  smooth loss = 0.4458
epoch 0 iter 5200: loss = 0.4252,  smooth loss = 0.4428
epoch 0 iter 5250: loss = 0.4557,  smooth loss = 0.4452
epoch 0 iter 5300: loss = 0.4437,  smooth loss = 0.4450
epoch 0 iter 5350: loss = 0.4458,  smooth loss = 0.4395
epoch 0 iter 5400: loss = 0.4229,  smooth loss = 0.4371
epoch 0 iter 5450: loss = 0.4410,  smooth loss = 0.4397
epoch 0 iter 5500: loss = 0.3985,  smooth loss = 0.4349
epoch 0 iter 5550: loss = 0.4526,  smooth loss = 0.4355
epoch 0 iter 5600: loss = 0.4501,  smooth loss = 0.4332
epoch 0 iter 5650: loss = 0.4068,  smooth loss = 0.4304
epoch 0 iter 5700: loss = 0.4420,  smooth loss = 0.4297
epoch 0 iter 5750: loss = 0.4064,  smooth loss = 0.4297
epoch 0 iter 5800: loss = 0.4084,  smooth loss = 0.4262
epoch 0 iter 5850: loss = 0.3941,  smooth loss = 0.4268
epoch 0 iter 5900: loss = 0.4156,  smooth loss = 0.4232
epoch 0 iter 5950: loss = 0.4261,  smooth loss = 0.4227
epoch 0 iter 6000: loss = 0.4157,  smooth loss = 0.4202
average data time = 0.0040s, average running time = 0.8451s
epoch 0 iter 6000: eval loss = 1.8613,  ccr = 0.8354,  cwr = 0.4933,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.4933.
Save model pretrain-language-model_0_6000
epoch 0 iter 6050: loss = 0.4080,  smooth loss = 0.4195
epoch 0 iter 6100: loss = 0.4289,  smooth loss = 0.4193
epoch 0 iter 6150: loss = 0.4050,  smooth loss = 0.4176
epoch 0 iter 6200: loss = 0.4072,  smooth loss = 0.4152
epoch 0 iter 6250: loss = 0.4073,  smooth loss = 0.4142
epoch 0 iter 6300: loss = 0.3934,  smooth loss = 0.4150
epoch 0 iter 6350: loss = 0.4006,  smooth loss = 0.4120
epoch 0 iter 6400: loss = 0.3932,  smooth loss = 0.4100
epoch 0 iter 6450: loss = 0.3952,  smooth loss = 0.4123
epoch 0 iter 6500: loss = 0.4261,  smooth loss = 0.4101
epoch 0 iter 6550: loss = 0.3583,  smooth loss = 0.4079
epoch 0 iter 6600: loss = 0.4076,  smooth loss = 0.4074
epoch 0 iter 6650: loss = 0.3947,  smooth loss = 0.4054
epoch 0 iter 6700: loss = 0.3979,  smooth loss = 0.4053
epoch 0 iter 6750: loss = 0.3905,  smooth loss = 0.4025
epoch 0 iter 6800: loss = 0.3416,  smooth loss = 0.3994
epoch 0 iter 6850: loss = 0.3956,  smooth loss = 0.3984
epoch 0 iter 6900: loss = 0.3969,  smooth loss = 0.4001
epoch 0 iter 6950: loss = 0.3821,  smooth loss = 0.3961
epoch 0 iter 7000: loss = 0.3974,  smooth loss = 0.3980
Save model pretrain-language-model_0_7000
epoch 0 iter 7050: loss = 0.3751,  smooth loss = 0.3988
epoch 0 iter 7100: loss = 0.4154,  smooth loss = 0.3941
epoch 0 iter 7150: loss = 0.4042,  smooth loss = 0.3937
epoch 0 iter 7200: loss = 0.3649,  smooth loss = 0.3913
epoch 0 iter 7250: loss = 0.3775,  smooth loss = 0.3905
epoch 0 iter 7300: loss = 0.4164,  smooth loss = 0.3908
epoch 0 iter 7350: loss = 0.3810,  smooth loss = 0.3912
epoch 0 iter 7400: loss = 0.4031,  smooth loss = 0.3920
epoch 0 iter 7450: loss = 0.3755,  smooth loss = 0.3863
epoch 0 iter 7500: loss = 0.3669,  smooth loss = 0.3878
epoch 0 iter 7550: loss = 0.3682,  smooth loss = 0.3838
epoch 0 iter 7600: loss = 0.3789,  smooth loss = 0.3823
epoch 0 iter 7650: loss = 0.3742,  smooth loss = 0.3826
epoch 0 iter 7700: loss = 0.4206,  smooth loss = 0.3836
epoch 0 iter 7750: loss = 0.3811,  smooth loss = 0.3821
epoch 0 iter 7800: loss = 0.3869,  smooth loss = 0.3806
epoch 0 iter 7850: loss = 0.4065,  smooth loss = 0.3816
epoch 0 iter 7900: loss = 0.3858,  smooth loss = 0.3793
epoch 0 iter 7950: loss = 0.3759,  smooth loss = 0.3788
epoch 0 iter 8000: loss = 0.3602,  smooth loss = 0.3791
Save model pretrain-language-model_0_8000
epoch 0 iter 8050: loss = 0.3636,  smooth loss = 0.3747
epoch 0 iter 8100: loss = 0.3769,  smooth loss = 0.3758
epoch 0 iter 8150: loss = 0.3810,  smooth loss = 0.3750
epoch 0 iter 8200: loss = 0.3876,  smooth loss = 0.3768
epoch 0 iter 8250: loss = 0.3859,  smooth loss = 0.3778
epoch 0 iter 8300: loss = 0.3935,  smooth loss = 0.3766
epoch 0 iter 8350: loss = 0.3734,  smooth loss = 0.3745
epoch 0 iter 8400: loss = 0.3457,  smooth loss = 0.3736
epoch 0 iter 8450: loss = 0.3896,  smooth loss = 0.3717
epoch 0 iter 8500: loss = 0.3282,  smooth loss = 0.3732
epoch 0 iter 8550: loss = 0.3604,  smooth loss = 0.3719
epoch 0 iter 8600: loss = 0.3549,  smooth loss = 0.3711
epoch 0 iter 8650: loss = 0.3594,  smooth loss = 0.3697
epoch 0 iter 8700: loss = 0.3520,  smooth loss = 0.3671
epoch 0 iter 8750: loss = 0.3351,  smooth loss = 0.3650
epoch 0 iter 8800: loss = 0.3662,  smooth loss = 0.3675
epoch 0 iter 8850: loss = 0.3656,  smooth loss = 0.3651
epoch 0 iter 8900: loss = 0.3627,  smooth loss = 0.3655
epoch 0 iter 8950: loss = 0.3673,  smooth loss = 0.3677
epoch 0 iter 9000: loss = 0.3623,  smooth loss = 0.3652
average data time = 0.0029s, average running time = 0.8488s
epoch 0 iter 9000: eval loss = 1.9672,  ccr = 0.8324,  cwr = 0.4931,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_9000
epoch 0 iter 9050: loss = 0.3764,  smooth loss = 0.3631
epoch 0 iter 9100: loss = 0.3437,  smooth loss = 0.3625
epoch 0 iter 9150: loss = 0.3696,  smooth loss = 0.3613
epoch 0 iter 9200: loss = 0.3548,  smooth loss = 0.3622
epoch 0 iter 9250: loss = 0.3849,  smooth loss = 0.3612
epoch 0 iter 9300: loss = 0.3856,  smooth loss = 0.3626
epoch 0 iter 9350: loss = 0.3579,  smooth loss = 0.3601
epoch 0 iter 9400: loss = 0.3473,  smooth loss = 0.3611
epoch 0 iter 9450: loss = 0.3399,  smooth loss = 0.3570
epoch 0 iter 9500: loss = 0.3782,  smooth loss = 0.3578
epoch 0 iter 9550: loss = 0.3597,  smooth loss = 0.3557
epoch 0 iter 9600: loss = 0.3595,  smooth loss = 0.3572
epoch 0 iter 9650: loss = 0.3579,  smooth loss = 0.3555
epoch 0 iter 9700: loss = 0.3327,  smooth loss = 0.3562
epoch 0 iter 9750: loss = 0.3548,  smooth loss = 0.3555
epoch 0 iter 9800: loss = 0.3701,  smooth loss = 0.3546
epoch 0 iter 9850: loss = 0.3597,  smooth loss = 0.3543
epoch 0 iter 9900: loss = 0.3565,  smooth loss = 0.3526
epoch 0 iter 9950: loss = 0.3707,  smooth loss = 0.3514
epoch 0 iter 10000: loss = 0.3562,  smooth loss = 0.3517
Save model pretrain-language-model_0_10000
epoch 0 iter 10050: loss = 0.3355,  smooth loss = 0.3515
epoch 0 iter 10100: loss = 0.3680,  smooth loss = 0.3504
epoch 0 iter 10150: loss = 0.3450,  smooth loss = 0.3507
epoch 0 iter 10200: loss = 0.3449,  smooth loss = 0.3478
epoch 0 iter 10250: loss = 0.3307,  smooth loss = 0.3494
epoch 0 iter 10300: loss = 0.3346,  smooth loss = 0.3481
epoch 0 iter 10350: loss = 0.3519,  smooth loss = 0.3513
epoch 0 iter 10400: loss = 0.3279,  smooth loss = 0.3481
epoch 0 iter 10450: loss = 0.3546,  smooth loss = 0.3488
epoch 0 iter 10500: loss = 0.3480,  smooth loss = 0.3487
epoch 0 iter 10550: loss = 0.3304,  smooth loss = 0.3460
epoch 0 iter 10600: loss = 0.3297,  smooth loss = 0.3441
epoch 0 iter 10650: loss = 0.3536,  smooth loss = 0.3441
epoch 0 iter 10700: loss = 0.3720,  smooth loss = 0.3453
epoch 0 iter 10750: loss = 0.3444,  smooth loss = 0.3442
epoch 0 iter 10800: loss = 0.3442,  smooth loss = 0.3423
epoch 0 iter 10850: loss = 0.3541,  smooth loss = 0.3423
epoch 0 iter 10900: loss = 0.3222,  smooth loss = 0.3440
epoch 0 iter 10950: loss = 0.3564,  smooth loss = 0.3440
epoch 0 iter 11000: loss = 0.3457,  smooth loss = 0.3397
Save model pretrain-language-model_0_11000
epoch 0 iter 11050: loss = 0.3583,  smooth loss = 0.3422
epoch 0 iter 11100: loss = 0.3658,  smooth loss = 0.3415
epoch 0 iter 11150: loss = 0.3161,  smooth loss = 0.3398
epoch 0 iter 11200: loss = 0.3413,  smooth loss = 0.3403
epoch 0 iter 11250: loss = 0.3406,  smooth loss = 0.3380
epoch 0 iter 11300: loss = 0.3221,  smooth loss = 0.3397
epoch 0 iter 11350: loss = 0.3395,  smooth loss = 0.3390
epoch 0 iter 11400: loss = 0.3378,  smooth loss = 0.3367
epoch 0 iter 11450: loss = 0.3226,  smooth loss = 0.3373
epoch 0 iter 11500: loss = 0.3538,  smooth loss = 0.3382
epoch 0 iter 11550: loss = 0.3287,  smooth loss = 0.3364
epoch 0 iter 11600: loss = 0.3280,  smooth loss = 0.3380
epoch 0 iter 11650: loss = 0.3231,  smooth loss = 0.3368
epoch 0 iter 11700: loss = 0.3118,  smooth loss = 0.3350
epoch 0 iter 11750: loss = 0.3199,  smooth loss = 0.3354
epoch 0 iter 11800: loss = 0.3195,  smooth loss = 0.3344
epoch 0 iter 11850: loss = 0.3529,  smooth loss = 0.3352
epoch 0 iter 11900: loss = 0.3197,  smooth loss = 0.3331
epoch 0 iter 11950: loss = 0.3208,  smooth loss = 0.3329
epoch 0 iter 12000: loss = 0.3117,  smooth loss = 0.3325
average data time = 0.0024s, average running time = 0.8516s
epoch 0 iter 12000: eval loss = 2.0222,  ccr = 0.8343,  cwr = 0.4894,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_12000
epoch 0 iter 12050: loss = 0.3455,  smooth loss = 0.3318
epoch 0 iter 12100: loss = 0.3262,  smooth loss = 0.3328
epoch 0 iter 12150: loss = 0.3362,  smooth loss = 0.3307
epoch 0 iter 12200: loss = 0.3267,  smooth loss = 0.3284
epoch 0 iter 12250: loss = 0.3084,  smooth loss = 0.3316
epoch 0 iter 12300: loss = 0.3317,  smooth loss = 0.3312
epoch 0 iter 12350: loss = 0.3113,  smooth loss = 0.3275
epoch 0 iter 12400: loss = 0.3284,  smooth loss = 0.3290
epoch 0 iter 12450: loss = 0.3399,  smooth loss = 0.3291
epoch 0 iter 12500: loss = 0.3396,  smooth loss = 0.3277
epoch 0 iter 12550: loss = 0.3173,  smooth loss = 0.3310
epoch 0 iter 12600: loss = 0.3205,  smooth loss = 0.3284
epoch 0 iter 12650: loss = 0.3220,  smooth loss = 0.3278
epoch 0 iter 12700: loss = 0.3431,  smooth loss = 0.3260
epoch 0 iter 12750: loss = 0.3323,  smooth loss = 0.3278
epoch 0 iter 12800: loss = 0.3400,  smooth loss = 0.3270
epoch 0 iter 12850: loss = 0.3475,  smooth loss = 0.3263
epoch 0 iter 12900: loss = 0.3354,  smooth loss = 0.3275
epoch 0 iter 12950: loss = 0.3315,  smooth loss = 0.3284
epoch 0 iter 13000: loss = 0.3287,  smooth loss = 0.3274
Save model pretrain-language-model_0_13000
epoch 0 iter 13050: loss = 0.3332,  smooth loss = 0.3235
epoch 0 iter 13100: loss = 0.3045,  smooth loss = 0.3230
epoch 0 iter 13150: loss = 0.3308,  smooth loss = 0.3267
epoch 0 iter 13200: loss = 0.3345,  smooth loss = 0.3265
epoch 0 iter 13250: loss = 0.3499,  smooth loss = 0.3253
epoch 0 iter 13300: loss = 0.3244,  smooth loss = 0.3246
epoch 0 iter 13350: loss = 0.3332,  smooth loss = 0.3255
epoch 0 iter 13400: loss = 0.3197,  smooth loss = 0.3236
epoch 0 iter 13450: loss = 0.3224,  smooth loss = 0.3220
epoch 0 iter 13500: loss = 0.3210,  smooth loss = 0.3218
epoch 0 iter 13550: loss = 0.3391,  smooth loss = 0.3193
epoch 0 iter 13600: loss = 0.3000,  smooth loss = 0.3195
epoch 0 iter 13650: loss = 0.3275,  smooth loss = 0.3203
epoch 0 iter 13700: loss = 0.3056,  smooth loss = 0.3226
epoch 0 iter 13750: loss = 0.3119,  smooth loss = 0.3219
epoch 0 iter 13800: loss = 0.3273,  smooth loss = 0.3190
epoch 0 iter 13850: loss = 0.3391,  smooth loss = 0.3234
epoch 0 iter 13900: loss = 0.3069,  smooth loss = 0.3187
epoch 0 iter 13950: loss = 0.3032,  smooth loss = 0.3206
epoch 0 iter 14000: loss = 0.3148,  smooth loss = 0.3194
Save model pretrain-language-model_0_14000
epoch 0 iter 14050: loss = 0.3221,  smooth loss = 0.3187
epoch 0 iter 14100: loss = 0.3074,  smooth loss = 0.3178
epoch 0 iter 14150: loss = 0.2955,  smooth loss = 0.3165
epoch 0 iter 14200: loss = 0.2964,  smooth loss = 0.3150
epoch 0 iter 14250: loss = 0.3154,  smooth loss = 0.3178
epoch 0 iter 14300: loss = 0.3277,  smooth loss = 0.3167
epoch 0 iter 14350: loss = 0.3069,  smooth loss = 0.3159
epoch 0 iter 14400: loss = 0.2888,  smooth loss = 0.3167
epoch 0 iter 14450: loss = 0.3158,  smooth loss = 0.3155
epoch 0 iter 14500: loss = 0.3503,  smooth loss = 0.3164
epoch 0 iter 14550: loss = 0.3052,  smooth loss = 0.3138
epoch 0 iter 14600: loss = 0.3036,  smooth loss = 0.3115
epoch 0 iter 14650: loss = 0.3025,  smooth loss = 0.3114
epoch 0 iter 14700: loss = 0.3206,  smooth loss = 0.3137
epoch 0 iter 14750: loss = 0.3203,  smooth loss = 0.3141
epoch 0 iter 14800: loss = 0.2947,  smooth loss = 0.3137
epoch 0 iter 14850: loss = 0.3027,  smooth loss = 0.3140
epoch 0 iter 14900: loss = 0.3206,  smooth loss = 0.3144
epoch 0 iter 14950: loss = 0.3274,  smooth loss = 0.3164
epoch 0 iter 15000: loss = 0.3272,  smooth loss = 0.3136
average data time = 0.0021s, average running time = 0.8558s
epoch 0 iter 15000: eval loss = 2.0800,  ccr = 0.8326,  cwr = 0.4905,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_15000
epoch 0 iter 15050: loss = 0.2963,  smooth loss = 0.3098
epoch 0 iter 15100: loss = 0.3037,  smooth loss = 0.3122
epoch 0 iter 15150: loss = 0.3121,  smooth loss = 0.3114
epoch 0 iter 15200: loss = 0.2865,  smooth loss = 0.3095
epoch 0 iter 15250: loss = 0.3147,  smooth loss = 0.3107
epoch 0 iter 15300: loss = 0.3164,  smooth loss = 0.3112
epoch 0 iter 15350: loss = 0.3160,  smooth loss = 0.3125
epoch 0 iter 15400: loss = 0.3144,  smooth loss = 0.3114
epoch 0 iter 15450: loss = 0.3097,  smooth loss = 0.3122
epoch 0 iter 15500: loss = 0.3068,  smooth loss = 0.3107
epoch 0 iter 15550: loss = 0.3142,  smooth loss = 0.3139
epoch 0 iter 15600: loss = 0.3389,  smooth loss = 0.3128
epoch 0 iter 15650: loss = 0.3170,  smooth loss = 0.3107
epoch 0 iter 15700: loss = 0.3095,  smooth loss = 0.3100
epoch 0 iter 15750: loss = 0.2887,  smooth loss = 0.3080
epoch 0 iter 15800: loss = 0.3059,  smooth loss = 0.3088
epoch 0 iter 15850: loss = 0.3235,  smooth loss = 0.3082
epoch 0 iter 15900: loss = 0.2848,  smooth loss = 0.3087
epoch 0 iter 15950: loss = 0.3280,  smooth loss = 0.3103
epoch 0 iter 16000: loss = 0.2815,  smooth loss = 0.3071
Save model pretrain-language-model_0_16000
epoch 0 iter 16050: loss = 0.3098,  smooth loss = 0.3065
epoch 0 iter 16100: loss = 0.3101,  smooth loss = 0.3085
epoch 0 iter 16150: loss = 0.3043,  smooth loss = 0.3069
epoch 0 iter 16200: loss = 0.2969,  smooth loss = 0.3074
epoch 0 iter 16250: loss = 0.3294,  smooth loss = 0.3081
epoch 0 iter 16300: loss = 0.3177,  smooth loss = 0.3087
epoch 0 iter 16350: loss = 0.2904,  smooth loss = 0.3066
epoch 0 iter 16400: loss = 0.3328,  smooth loss = 0.3043
epoch 0 iter 16450: loss = 0.3187,  smooth loss = 0.3050
epoch 0 iter 16500: loss = 0.3031,  smooth loss = 0.3050
epoch 0 iter 16550: loss = 0.3143,  smooth loss = 0.3080
epoch 0 iter 16600: loss = 0.3031,  smooth loss = 0.3058
epoch 0 iter 16650: loss = 0.2897,  smooth loss = 0.3046
epoch 0 iter 16700: loss = 0.3073,  smooth loss = 0.3055
epoch 0 iter 16750: loss = 0.2942,  smooth loss = 0.3066
epoch 0 iter 16800: loss = 0.3073,  smooth loss = 0.3051
epoch 0 iter 16850: loss = 0.2729,  smooth loss = 0.3023
epoch 0 iter 16900: loss = 0.3032,  smooth loss = 0.3051
epoch 0 iter 16950: loss = 0.3078,  smooth loss = 0.3032
epoch 0 iter 17000: loss = 0.2862,  smooth loss = 0.3015
Save model pretrain-language-model_0_17000
epoch 0 iter 17050: loss = 0.3447,  smooth loss = 0.3042
epoch 0 iter 17100: loss = 0.2762,  smooth loss = 0.3030
epoch 0 iter 17150: loss = 0.3103,  smooth loss = 0.3033
epoch 0 iter 17200: loss = 0.3067,  smooth loss = 0.3054
epoch 0 iter 17250: loss = 0.3078,  smooth loss = 0.3031
epoch 0 iter 17300: loss = 0.2983,  smooth loss = 0.3031
epoch 0 iter 17350: loss = 0.3053,  smooth loss = 0.3006
epoch 0 iter 17400: loss = 0.2871,  smooth loss = 0.3019
epoch 0 iter 17450: loss = 0.3093,  smooth loss = 0.3021
epoch 0 iter 17500: loss = 0.3044,  smooth loss = 0.3015
epoch 0 iter 17550: loss = 0.2950,  smooth loss = 0.3038
epoch 0 iter 17600: loss = 0.3008,  smooth loss = 0.3006
epoch 0 iter 17650: loss = 0.2905,  smooth loss = 0.2997
epoch 0 iter 17700: loss = 0.2931,  smooth loss = 0.2977
epoch 0 iter 17750: loss = 0.3178,  smooth loss = 0.2987
epoch 0 iter 17800: loss = 0.2857,  smooth loss = 0.2998
epoch 0 iter 17850: loss = 0.2978,  smooth loss = 0.2987
epoch 0 iter 17900: loss = 0.3024,  smooth loss = 0.2991
epoch 0 iter 17950: loss = 0.2982,  smooth loss = 0.2976
epoch 0 iter 18000: loss = 0.2967,  smooth loss = 0.2990
average data time = 0.0019s, average running time = 0.8570s
epoch 0 iter 18000: eval loss = 2.0945,  ccr = 0.8342,  cwr = 0.4927,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_18000
epoch 0 iter 18050: loss = 0.3093,  smooth loss = 0.2995
epoch 0 iter 18100: loss = 0.2959,  smooth loss = 0.2976
epoch 0 iter 18150: loss = 0.3048,  smooth loss = 0.2999
epoch 0 iter 18200: loss = 0.3044,  smooth loss = 0.2980
epoch 0 iter 18250: loss = 0.2944,  smooth loss = 0.2969
epoch 0 iter 18300: loss = 0.3045,  smooth loss = 0.2977
epoch 0 iter 18350: loss = 0.2874,  smooth loss = 0.2962
epoch 0 iter 18400: loss = 0.2907,  smooth loss = 0.2978
epoch 0 iter 18450: loss = 0.2901,  smooth loss = 0.2980
epoch 0 iter 18500: loss = 0.2726,  smooth loss = 0.2966
epoch 0 iter 18550: loss = 0.3213,  smooth loss = 0.2974
epoch 0 iter 18600: loss = 0.2943,  smooth loss = 0.2973
epoch 0 iter 18650: loss = 0.3244,  smooth loss = 0.2977
epoch 0 iter 18700: loss = 0.3018,  smooth loss = 0.2949
epoch 0 iter 18750: loss = 0.2774,  smooth loss = 0.2941
epoch 0 iter 18800: loss = 0.3042,  smooth loss = 0.2955
epoch 0 iter 18850: loss = 0.2861,  smooth loss = 0.2946
epoch 0 iter 18900: loss = 0.2804,  smooth loss = 0.2966
epoch 0 iter 18950: loss = 0.3281,  smooth loss = 0.2978
epoch 0 iter 19000: loss = 0.3079,  smooth loss = 0.2965
Save model pretrain-language-model_0_19000
epoch 0 iter 19050: loss = 0.3131,  smooth loss = 0.2958
epoch 0 iter 19100: loss = 0.2842,  smooth loss = 0.2966
epoch 0 iter 19150: loss = 0.3026,  smooth loss = 0.2955
epoch 0 iter 19200: loss = 0.2742,  smooth loss = 0.2954
epoch 0 iter 19250: loss = 0.2992,  smooth loss = 0.2956
epoch 0 iter 19300: loss = 0.3099,  smooth loss = 0.2934
epoch 0 iter 19350: loss = 0.2598,  smooth loss = 0.2932
epoch 0 iter 19400: loss = 0.2806,  smooth loss = 0.2926
epoch 0 iter 19450: loss = 0.2994,  smooth loss = 0.2939
epoch 0 iter 19500: loss = 0.2947,  smooth loss = 0.2934
epoch 0 iter 19550: loss = 0.3138,  smooth loss = 0.2930
epoch 0 iter 19600: loss = 0.3113,  smooth loss = 0.2940
epoch 0 iter 19650: loss = 0.2968,  smooth loss = 0.2947
epoch 0 iter 19700: loss = 0.3065,  smooth loss = 0.2951
epoch 0 iter 19750: loss = 0.2942,  smooth loss = 0.2937
epoch 0 iter 19800: loss = 0.2971,  smooth loss = 0.2948
epoch 0 iter 19850: loss = 0.2733,  smooth loss = 0.2942
epoch 0 iter 19900: loss = 0.2890,  smooth loss = 0.2934
epoch 0 iter 19950: loss = 0.2963,  smooth loss = 0.2934
epoch 0 iter 20000: loss = 0.2928,  smooth loss = 0.2915
Save model pretrain-language-model_0_20000
epoch 0 iter 20050: loss = 0.2816,  smooth loss = 0.2911
epoch 0 iter 20100: loss = 0.3023,  smooth loss = 0.2924
epoch 0 iter 20150: loss = 0.2945,  smooth loss = 0.2917
epoch 0 iter 20200: loss = 0.2754,  smooth loss = 0.2898
epoch 0 iter 20250: loss = 0.2951,  smooth loss = 0.2904
epoch 0 iter 20300: loss = 0.3252,  smooth loss = 0.2933
epoch 0 iter 20350: loss = 0.3198,  smooth loss = 0.2935
epoch 0 iter 20400: loss = 0.2889,  smooth loss = 0.2929
epoch 0 iter 20450: loss = 0.2801,  smooth loss = 0.2915
epoch 0 iter 20500: loss = 0.2936,  smooth loss = 0.2925
epoch 0 iter 20550: loss = 0.2634,  smooth loss = 0.2898
epoch 0 iter 20600: loss = 0.2962,  smooth loss = 0.2906
epoch 0 iter 20650: loss = 0.2906,  smooth loss = 0.2906
epoch 0 iter 20700: loss = 0.2931,  smooth loss = 0.2888
epoch 0 iter 20750: loss = 0.2829,  smooth loss = 0.2884
epoch 0 iter 20800: loss = 0.2763,  smooth loss = 0.2885
epoch 0 iter 20850: loss = 0.2978,  smooth loss = 0.2882
epoch 0 iter 20900: loss = 0.2784,  smooth loss = 0.2888
epoch 0 iter 20950: loss = 0.2916,  smooth loss = 0.2892
epoch 0 iter 21000: loss = 0.3061,  smooth loss = 0.2889
average data time = 0.0018s, average running time = 0.8588s
epoch 0 iter 21000: eval loss = 2.0937,  ccr = 0.8343,  cwr = 0.4918,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_21000
epoch 0 iter 21050: loss = 0.2664,  smooth loss = 0.2889
epoch 0 iter 21100: loss = 0.3024,  smooth loss = 0.2879
epoch 0 iter 21150: loss = 0.2840,  smooth loss = 0.2891
epoch 0 iter 21200: loss = 0.2969,  smooth loss = 0.2877
epoch 0 iter 21250: loss = 0.2927,  smooth loss = 0.2895
epoch 0 iter 21300: loss = 0.2710,  smooth loss = 0.2885
epoch 0 iter 21350: loss = 0.2602,  smooth loss = 0.2875
epoch 0 iter 21400: loss = 0.3011,  smooth loss = 0.2891
epoch 0 iter 21450: loss = 0.2571,  smooth loss = 0.2913
epoch 0 iter 21500: loss = 0.3099,  smooth loss = 0.2891
epoch 0 iter 21550: loss = 0.2725,  smooth loss = 0.2878
epoch 0 iter 21600: loss = 0.2965,  smooth loss = 0.2897
epoch 0 iter 21650: loss = 0.2977,  smooth loss = 0.2881
epoch 0 iter 21700: loss = 0.3030,  smooth loss = 0.2866
epoch 0 iter 21750: loss = 0.2863,  smooth loss = 0.2878
epoch 0 iter 21800: loss = 0.2857,  smooth loss = 0.2867
epoch 0 iter 21850: loss = 0.2669,  smooth loss = 0.2865
epoch 0 iter 21900: loss = 0.2799,  smooth loss = 0.2858
epoch 0 iter 21950: loss = 0.2680,  smooth loss = 0.2867
epoch 0 iter 22000: loss = 0.2886,  smooth loss = 0.2892
Save model pretrain-language-model_0_22000
epoch 0 iter 22050: loss = 0.3009,  smooth loss = 0.2865
epoch 0 iter 22100: loss = 0.2929,  smooth loss = 0.2842
epoch 0 iter 22150: loss = 0.2917,  smooth loss = 0.2865
epoch 0 iter 22200: loss = 0.2956,  smooth loss = 0.2875
epoch 0 iter 22250: loss = 0.2797,  smooth loss = 0.2863
epoch 0 iter 22300: loss = 0.3026,  smooth loss = 0.2845
epoch 0 iter 22350: loss = 0.2741,  smooth loss = 0.2820
epoch 0 iter 22400: loss = 0.2899,  smooth loss = 0.2839
epoch 0 iter 22450: loss = 0.2773,  smooth loss = 0.2858
epoch 0 iter 22500: loss = 0.2627,  smooth loss = 0.2845
epoch 0 iter 22550: loss = 0.3024,  smooth loss = 0.2828
epoch 0 iter 22600: loss = 0.2973,  smooth loss = 0.2829
epoch 0 iter 22650: loss = 0.2926,  smooth loss = 0.2834
epoch 0 iter 22700: loss = 0.2706,  smooth loss = 0.2846
epoch 0 iter 22750: loss = 0.2746,  smooth loss = 0.2855
epoch 0 iter 22800: loss = 0.2872,  smooth loss = 0.2858
epoch 0 iter 22850: loss = 0.2651,  smooth loss = 0.2840
epoch 0 iter 22900: loss = 0.2839,  smooth loss = 0.2824
epoch 0 iter 22950: loss = 0.3029,  smooth loss = 0.2823
epoch 0 iter 23000: loss = 0.3091,  smooth loss = 0.2830
Save model pretrain-language-model_0_23000
epoch 0 iter 23050: loss = 0.2770,  smooth loss = 0.2824
epoch 0 iter 23100: loss = 0.2720,  smooth loss = 0.2835
epoch 0 iter 23150: loss = 0.2880,  smooth loss = 0.2836
epoch 0 iter 23200: loss = 0.2943,  smooth loss = 0.2837
epoch 0 iter 23250: loss = 0.2867,  smooth loss = 0.2840
epoch 0 iter 23300: loss = 0.3162,  smooth loss = 0.2829
epoch 0 iter 23350: loss = 0.2875,  smooth loss = 0.2840
epoch 0 iter 23400: loss = 0.2599,  smooth loss = 0.2825
epoch 0 iter 23450: loss = 0.2863,  smooth loss = 0.2810
epoch 0 iter 23500: loss = 0.2830,  smooth loss = 0.2816
epoch 0 iter 23550: loss = 0.2704,  smooth loss = 0.2803
epoch 0 iter 23600: loss = 0.2715,  smooth loss = 0.2796
epoch 0 iter 23650: loss = 0.2785,  smooth loss = 0.2813
epoch 0 iter 23700: loss = 0.2681,  smooth loss = 0.2789
epoch 0 iter 23750: loss = 0.2854,  smooth loss = 0.2800
epoch 0 iter 23800: loss = 0.2728,  smooth loss = 0.2832
epoch 0 iter 23850: loss = 0.2779,  smooth loss = 0.2826
epoch 0 iter 23900: loss = 0.2853,  smooth loss = 0.2814
epoch 0 iter 23950: loss = 0.2707,  smooth loss = 0.2811
epoch 0 iter 24000: loss = 0.2612,  smooth loss = 0.2791
average data time = 0.0017s, average running time = 0.8601s
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Use 2 GPUs.
Start training.
epoch 0 iter 50: loss = 2.2284,  smooth loss = 2.4343
epoch 0 iter 100: loss = 2.0063,  smooth loss = 2.1887
epoch 0 iter 150: loss = 1.7745,  smooth loss = 1.9687
epoch 0 iter 200: loss = 1.6848,  smooth loss = 1.7946
epoch 0 iter 250: loss = 1.5645,  smooth loss = 1.6743
epoch 0 iter 300: loss = 1.4850,  smooth loss = 1.5817
epoch 0 iter 350: loss = 1.4410,  smooth loss = 1.5077
epoch 0 iter 400: loss = 1.3847,  smooth loss = 1.4429
epoch 0 iter 450: loss = 1.3086,  smooth loss = 1.3831
epoch 0 iter 500: loss = 1.2969,  smooth loss = 1.3347
epoch 0 iter 550: loss = 1.2392,  smooth loss = 1.2901
epoch 0 iter 600: loss = 1.2327,  smooth loss = 1.2501
epoch 0 iter 650: loss = 1.1761,  smooth loss = 1.2110
epoch 0 iter 700: loss = 1.1255,  smooth loss = 1.1792
epoch 0 iter 750: loss = 1.1478,  smooth loss = 1.1504
epoch 0 iter 800: loss = 1.0777,  smooth loss = 1.1185
epoch 0 iter 850: loss = 1.0682,  smooth loss = 1.0877
epoch 0 iter 900: loss = 1.0488,  smooth loss = 1.0650
epoch 0 iter 950: loss = 1.0143,  smooth loss = 1.0386
epoch 0 iter 1000: loss = 1.0152,  smooth loss = 1.0158
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9764,  smooth loss = 0.9879
epoch 0 iter 1100: loss = 0.9575,  smooth loss = 0.9658
epoch 0 iter 1150: loss = 0.9472,  smooth loss = 0.9438
epoch 0 iter 1200: loss = 0.9400,  smooth loss = 0.9261
epoch 0 iter 1250: loss = 0.8851,  smooth loss = 0.9080
epoch 0 iter 1300: loss = 0.8782,  smooth loss = 0.8882
epoch 0 iter 1350: loss = 0.8693,  smooth loss = 0.8696
epoch 0 iter 1400: loss = 0.8525,  smooth loss = 0.8513
epoch 0 iter 1450: loss = 0.8337,  smooth loss = 0.8396
epoch 0 iter 1500: loss = 0.8127,  smooth loss = 0.8237
epoch 0 iter 1550: loss = 0.7839,  smooth loss = 0.8088
epoch 0 iter 1600: loss = 0.7776,  smooth loss = 0.7929
epoch 0 iter 1650: loss = 0.7895,  smooth loss = 0.7812
epoch 0 iter 1700: loss = 0.7641,  smooth loss = 0.7711
epoch 0 iter 1750: loss = 0.7774,  smooth loss = 0.7580
epoch 0 iter 1800: loss = 0.7114,  smooth loss = 0.7437
epoch 0 iter 1850: loss = 0.7451,  smooth loss = 0.7354
epoch 0 iter 1900: loss = 0.7044,  smooth loss = 0.7241
epoch 0 iter 1950: loss = 0.7622,  smooth loss = 0.7145
epoch 0 iter 2000: loss = 0.6907,  smooth loss = 0.7011
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.6830,  smooth loss = 0.6915
epoch 0 iter 2100: loss = 0.6543,  smooth loss = 0.6834
epoch 0 iter 2150: loss = 0.6916,  smooth loss = 0.6767
epoch 0 iter 2200: loss = 0.6720,  smooth loss = 0.6683
epoch 0 iter 2250: loss = 0.6711,  smooth loss = 0.6591
epoch 0 iter 2300: loss = 0.6261,  smooth loss = 0.6488
epoch 0 iter 2350: loss = 0.6426,  smooth loss = 0.6433
epoch 0 iter 2400: loss = 0.6237,  smooth loss = 0.6373
epoch 0 iter 2450: loss = 0.6297,  smooth loss = 0.6308
epoch 0 iter 2500: loss = 0.6443,  smooth loss = 0.6272
epoch 0 iter 2550: loss = 0.6322,  smooth loss = 0.6223
epoch 0 iter 2600: loss = 0.6135,  smooth loss = 0.6156
epoch 0 iter 2650: loss = 0.5801,  smooth loss = 0.6095
epoch 0 iter 2700: loss = 0.5932,  smooth loss = 0.6039
epoch 0 iter 2750: loss = 0.6037,  smooth loss = 0.5978
epoch 0 iter 2800: loss = 0.5776,  smooth loss = 0.5913
epoch 0 iter 2850: loss = 0.5897,  smooth loss = 0.5830
epoch 0 iter 2900: loss = 0.5841,  smooth loss = 0.5802
epoch 0 iter 2950: loss = 0.5492,  smooth loss = 0.5724
epoch 0 iter 3000: loss = 0.5838,  smooth loss = 0.5699
average data time = 0.0090s, average running time = 0.5308s
epoch 0 iter 3000: eval loss = 1.8475,  ccr = 0.8357,  cwr = 0.4847,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4847.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5704,  smooth loss = 0.5655
epoch 0 iter 3100: loss = 0.5693,  smooth loss = 0.5628
epoch 0 iter 3150: loss = 0.5640,  smooth loss = 0.5581
epoch 0 iter 3200: loss = 0.5452,  smooth loss = 0.5512
epoch 0 iter 3250: loss = 0.5335,  smooth loss = 0.5477
epoch 0 iter 3300: loss = 0.5691,  smooth loss = 0.5443
epoch 0 iter 3350: loss = 0.5410,  smooth loss = 0.5388
epoch 0 iter 3400: loss = 0.5291,  smooth loss = 0.5374
epoch 0 iter 3450: loss = 0.5469,  smooth loss = 0.5313
epoch 0 iter 3500: loss = 0.5406,  smooth loss = 0.5281
epoch 0 iter 3550: loss = 0.5382,  smooth loss = 0.5283
epoch 0 iter 3600: loss = 0.5112,  smooth loss = 0.5236
epoch 0 iter 3650: loss = 0.4993,  smooth loss = 0.5189
epoch 0 iter 3700: loss = 0.4990,  smooth loss = 0.5209
epoch 0 iter 3750: loss = 0.4877,  smooth loss = 0.5130
epoch 0 iter 3800: loss = 0.5093,  smooth loss = 0.5078
epoch 0 iter 3850: loss = 0.4909,  smooth loss = 0.5063
epoch 0 iter 3900: loss = 0.4704,  smooth loss = 0.5013
epoch 0 iter 3950: loss = 0.5139,  smooth loss = 0.4997
epoch 0 iter 4000: loss = 0.4678,  smooth loss = 0.4977
Save model pretrain-language-model_0_4000
epoch 0 iter 4050: loss = 0.5334,  smooth loss = 0.4951
epoch 0 iter 4100: loss = 0.5101,  smooth loss = 0.4920
epoch 0 iter 4150: loss = 0.4979,  smooth loss = 0.4899
epoch 0 iter 4200: loss = 0.5217,  smooth loss = 0.4875
epoch 0 iter 4250: loss = 0.4912,  smooth loss = 0.4849
epoch 0 iter 4300: loss = 0.5025,  smooth loss = 0.4828
epoch 0 iter 4350: loss = 0.4707,  smooth loss = 0.4785
epoch 0 iter 4400: loss = 0.4895,  smooth loss = 0.4792
epoch 0 iter 4450: loss = 0.4791,  smooth loss = 0.4782
epoch 0 iter 4500: loss = 0.4662,  smooth loss = 0.4721
epoch 0 iter 4550: loss = 0.4460,  smooth loss = 0.4694
epoch 0 iter 4600: loss = 0.4672,  smooth loss = 0.4653
epoch 0 iter 4650: loss = 0.4806,  smooth loss = 0.4659
epoch 0 iter 4700: loss = 0.4896,  smooth loss = 0.4660
epoch 0 iter 4750: loss = 0.4872,  smooth loss = 0.4626
epoch 0 iter 4800: loss = 0.4890,  smooth loss = 0.4589
epoch 0 iter 4850: loss = 0.4549,  smooth loss = 0.4574
epoch 0 iter 4900: loss = 0.4597,  smooth loss = 0.4580
epoch 0 iter 4950: loss = 0.4531,  smooth loss = 0.4551
epoch 0 iter 5000: loss = 0.4687,  smooth loss = 0.4548
Save model pretrain-language-model_0_5000
epoch 0 iter 5050: loss = 0.4501,  smooth loss = 0.4514
epoch 0 iter 5100: loss = 0.4491,  smooth loss = 0.4487
epoch 0 iter 5150: loss = 0.4389,  smooth loss = 0.4450
epoch 0 iter 5200: loss = 0.4702,  smooth loss = 0.4458
epoch 0 iter 5250: loss = 0.4072,  smooth loss = 0.4454
epoch 0 iter 5300: loss = 0.4519,  smooth loss = 0.4420
epoch 0 iter 5350: loss = 0.4683,  smooth loss = 0.4419
epoch 0 iter 5400: loss = 0.4355,  smooth loss = 0.4378
epoch 0 iter 5450: loss = 0.4293,  smooth loss = 0.4364
epoch 0 iter 5500: loss = 0.4308,  smooth loss = 0.4361
epoch 0 iter 5550: loss = 0.4739,  smooth loss = 0.4330
epoch 0 iter 5600: loss = 0.4205,  smooth loss = 0.4306
epoch 0 iter 5650: loss = 0.4493,  smooth loss = 0.4318
epoch 0 iter 5700: loss = 0.4569,  smooth loss = 0.4287
epoch 0 iter 5750: loss = 0.4097,  smooth loss = 0.4268
epoch 0 iter 5800: loss = 0.4209,  smooth loss = 0.4264
epoch 0 iter 5850: loss = 0.3911,  smooth loss = 0.4243
epoch 0 iter 5900: loss = 0.4310,  smooth loss = 0.4223
epoch 0 iter 5950: loss = 0.4063,  smooth loss = 0.4216
epoch 0 iter 6000: loss = 0.4306,  smooth loss = 0.4228
average data time = 0.0056s, average running time = 0.5362s
epoch 0 iter 6000: eval loss = 1.9167,  ccr = 0.8325,  cwr = 0.4887,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.4887.
Save model pretrain-language-model_0_6000
epoch 0 iter 6050: loss = 0.4133,  smooth loss = 0.4209
epoch 0 iter 6100: loss = 0.4281,  smooth loss = 0.4196
epoch 0 iter 6150: loss = 0.4006,  smooth loss = 0.4173
epoch 0 iter 6200: loss = 0.4161,  smooth loss = 0.4160
epoch 0 iter 6250: loss = 0.4122,  smooth loss = 0.4142
epoch 0 iter 6300: loss = 0.3953,  smooth loss = 0.4135
epoch 0 iter 6350: loss = 0.4212,  smooth loss = 0.4153
epoch 0 iter 6400: loss = 0.4244,  smooth loss = 0.4131
epoch 0 iter 6450: loss = 0.3978,  smooth loss = 0.4108
epoch 0 iter 6500: loss = 0.4227,  smooth loss = 0.4096
epoch 0 iter 6550: loss = 0.4222,  smooth loss = 0.4090
epoch 0 iter 6600: loss = 0.4397,  smooth loss = 0.4080
epoch 0 iter 6650: loss = 0.4058,  smooth loss = 0.4060
epoch 0 iter 6700: loss = 0.3916,  smooth loss = 0.4007
epoch 0 iter 6750: loss = 0.3878,  smooth loss = 0.3991
epoch 0 iter 6800: loss = 0.4244,  smooth loss = 0.4017
epoch 0 iter 6850: loss = 0.4026,  smooth loss = 0.3998
epoch 0 iter 6900: loss = 0.4077,  smooth loss = 0.3966
epoch 0 iter 6950: loss = 0.4109,  smooth loss = 0.3957
epoch 0 iter 7000: loss = 0.3888,  smooth loss = 0.3959
Save model pretrain-language-model_0_7000
epoch 0 iter 7050: loss = 0.3986,  smooth loss = 0.3970
epoch 0 iter 7100: loss = 0.3731,  smooth loss = 0.3959
epoch 0 iter 7150: loss = 0.3808,  smooth loss = 0.3931
epoch 0 iter 7200: loss = 0.3771,  smooth loss = 0.3927
epoch 0 iter 7250: loss = 0.3702,  smooth loss = 0.3902
epoch 0 iter 7300: loss = 0.4241,  smooth loss = 0.3925
epoch 0 iter 7350: loss = 0.3778,  smooth loss = 0.3918
epoch 0 iter 7400: loss = 0.4124,  smooth loss = 0.3903
epoch 0 iter 7450: loss = 0.3935,  smooth loss = 0.3889
epoch 0 iter 7500: loss = 0.3925,  smooth loss = 0.3871
epoch 0 iter 7550: loss = 0.3832,  smooth loss = 0.3862
epoch 0 iter 7600: loss = 0.3626,  smooth loss = 0.3876
epoch 0 iter 7650: loss = 0.3882,  smooth loss = 0.3844
epoch 0 iter 7700: loss = 0.3716,  smooth loss = 0.3832
epoch 0 iter 7750: loss = 0.3668,  smooth loss = 0.3837
epoch 0 iter 7800: loss = 0.3933,  smooth loss = 0.3841
epoch 0 iter 7850: loss = 0.3805,  smooth loss = 0.3823
epoch 0 iter 7900: loss = 0.3657,  smooth loss = 0.3782
epoch 0 iter 7950: loss = 0.3703,  smooth loss = 0.3779
epoch 0 iter 8000: loss = 0.3588,  smooth loss = 0.3777
Save model pretrain-language-model_0_8000
epoch 0 iter 8050: loss = 0.4086,  smooth loss = 0.3805
epoch 0 iter 8100: loss = 0.3624,  smooth loss = 0.3790
epoch 0 iter 8150: loss = 0.3841,  smooth loss = 0.3753
epoch 0 iter 8200: loss = 0.3622,  smooth loss = 0.3745
epoch 0 iter 8250: loss = 0.3619,  smooth loss = 0.3752
epoch 0 iter 8300: loss = 0.3554,  smooth loss = 0.3732
epoch 0 iter 8350: loss = 0.3915,  smooth loss = 0.3714
epoch 0 iter 8400: loss = 0.4064,  smooth loss = 0.3741
epoch 0 iter 8450: loss = 0.3892,  smooth loss = 0.3750
epoch 0 iter 8500: loss = 0.3604,  smooth loss = 0.3719
epoch 0 iter 8550: loss = 0.3644,  smooth loss = 0.3724
epoch 0 iter 8600: loss = 0.3776,  smooth loss = 0.3740
epoch 0 iter 8650: loss = 0.3655,  smooth loss = 0.3681
epoch 0 iter 8700: loss = 0.3527,  smooth loss = 0.3680
epoch 0 iter 8750: loss = 0.3717,  smooth loss = 0.3662
epoch 0 iter 8800: loss = 0.3786,  smooth loss = 0.3669
epoch 0 iter 8850: loss = 0.3521,  smooth loss = 0.3681
epoch 0 iter 8900: loss = 0.3642,  smooth loss = 0.3671
epoch 0 iter 8950: loss = 0.3736,  smooth loss = 0.3681
epoch 0 iter 9000: loss = 0.3679,  smooth loss = 0.3664
average data time = 0.0045s, average running time = 0.5385s
epoch 0 iter 9000: eval loss = 1.9762,  ccr = 0.8329,  cwr = 0.4857,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_9000
epoch 0 iter 9050: loss = 0.3927,  smooth loss = 0.3678
epoch 0 iter 9100: loss = 0.3637,  smooth loss = 0.3661
epoch 0 iter 9150: loss = 0.3714,  smooth loss = 0.3650
epoch 0 iter 9200: loss = 0.3613,  smooth loss = 0.3624
epoch 0 iter 9250: loss = 0.3511,  smooth loss = 0.3617
epoch 0 iter 9300: loss = 0.3436,  smooth loss = 0.3595
epoch 0 iter 9350: loss = 0.3612,  smooth loss = 0.3598
epoch 0 iter 9400: loss = 0.3657,  smooth loss = 0.3598
epoch 0 iter 9450: loss = 0.3787,  smooth loss = 0.3588
epoch 0 iter 9500: loss = 0.3477,  smooth loss = 0.3574
epoch 0 iter 9550: loss = 0.3513,  smooth loss = 0.3596
epoch 0 iter 9600: loss = 0.3368,  smooth loss = 0.3571
epoch 0 iter 9650: loss = 0.3449,  smooth loss = 0.3562
epoch 0 iter 9700: loss = 0.3265,  smooth loss = 0.3567
epoch 0 iter 9750: loss = 0.3482,  smooth loss = 0.3552
epoch 0 iter 9800: loss = 0.3391,  smooth loss = 0.3550
epoch 0 iter 9850: loss = 0.3470,  smooth loss = 0.3549
epoch 0 iter 9900: loss = 0.3364,  smooth loss = 0.3533
epoch 0 iter 9950: loss = 0.3339,  smooth loss = 0.3517
epoch 0 iter 10000: loss = 0.3593,  smooth loss = 0.3516
Save model pretrain-language-model_0_10000
epoch 0 iter 10050: loss = 0.3793,  smooth loss = 0.3509
epoch 0 iter 10100: loss = 0.3332,  smooth loss = 0.3504
epoch 0 iter 10150: loss = 0.3551,  smooth loss = 0.3517
epoch 0 iter 10200: loss = 0.3645,  smooth loss = 0.3513
epoch 0 iter 10250: loss = 0.3462,  smooth loss = 0.3512
epoch 0 iter 10300: loss = 0.3408,  smooth loss = 0.3512
epoch 0 iter 10350: loss = 0.3399,  smooth loss = 0.3488
epoch 0 iter 10400: loss = 0.3467,  smooth loss = 0.3450
epoch 0 iter 10450: loss = 0.3336,  smooth loss = 0.3446
epoch 0 iter 10500: loss = 0.3658,  smooth loss = 0.3462
epoch 0 iter 10550: loss = 0.3427,  smooth loss = 0.3491
epoch 0 iter 10600: loss = 0.3649,  smooth loss = 0.3472
epoch 0 iter 10650: loss = 0.3350,  smooth loss = 0.3468
epoch 0 iter 10700: loss = 0.3388,  smooth loss = 0.3459
epoch 0 iter 10750: loss = 0.3384,  smooth loss = 0.3449
epoch 0 iter 10800: loss = 0.3433,  smooth loss = 0.3436
epoch 0 iter 10850: loss = 0.3254,  smooth loss = 0.3441
epoch 0 iter 10900: loss = 0.3855,  smooth loss = 0.3450
epoch 0 iter 10950: loss = 0.3556,  smooth loss = 0.3439
epoch 0 iter 11000: loss = 0.3604,  smooth loss = 0.3440
Save model pretrain-language-model_0_11000
epoch 0 iter 11050: loss = 0.3186,  smooth loss = 0.3451
epoch 0 iter 11100: loss = 0.3620,  smooth loss = 0.3435
epoch 0 iter 11150: loss = 0.3647,  smooth loss = 0.3441
epoch 0 iter 11200: loss = 0.3351,  smooth loss = 0.3410
epoch 0 iter 11250: loss = 0.3347,  smooth loss = 0.3410
epoch 0 iter 11300: loss = 0.3235,  smooth loss = 0.3394
epoch 0 iter 11350: loss = 0.3394,  smooth loss = 0.3397
epoch 0 iter 11400: loss = 0.3450,  smooth loss = 0.3373
epoch 0 iter 11450: loss = 0.3348,  smooth loss = 0.3369
epoch 0 iter 11500: loss = 0.3389,  smooth loss = 0.3359
epoch 0 iter 11550: loss = 0.3214,  smooth loss = 0.3380
epoch 0 iter 11600: loss = 0.3681,  smooth loss = 0.3377
epoch 0 iter 11650: loss = 0.3143,  smooth loss = 0.3358
epoch 0 iter 11700: loss = 0.3464,  smooth loss = 0.3392
epoch 0 iter 11750: loss = 0.3426,  smooth loss = 0.3384
epoch 0 iter 11800: loss = 0.3353,  smooth loss = 0.3358
epoch 0 iter 11850: loss = 0.3185,  smooth loss = 0.3342
epoch 0 iter 11900: loss = 0.3324,  smooth loss = 0.3353
epoch 0 iter 11950: loss = 0.3479,  smooth loss = 0.3358
epoch 0 iter 12000: loss = 0.3478,  smooth loss = 0.3359
average data time = 0.0039s, average running time = 0.5398s
epoch 0 iter 12000: eval loss = 2.0167,  ccr = 0.8308,  cwr = 0.4873,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_12000
epoch 0 iter 12050: loss = 0.3349,  smooth loss = 0.3364
epoch 0 iter 12100: loss = 0.3125,  smooth loss = 0.3350
epoch 0 iter 12150: loss = 0.3241,  smooth loss = 0.3332
epoch 0 iter 12200: loss = 0.3348,  smooth loss = 0.3334
epoch 0 iter 12250: loss = 0.3216,  smooth loss = 0.3328
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Use 2 GPUs.
Start training.
epoch 0 iter 50: loss = 2.2396,  smooth loss = 2.4341
epoch 0 iter 100: loss = 1.9427,  smooth loss = 2.1750
epoch 0 iter 150: loss = 1.7754,  smooth loss = 1.9632
epoch 0 iter 200: loss = 1.6436,  smooth loss = 1.7933
epoch 0 iter 250: loss = 1.5744,  smooth loss = 1.6731
epoch 0 iter 300: loss = 1.4920,  smooth loss = 1.5832
epoch 0 iter 350: loss = 1.4704,  smooth loss = 1.5128
epoch 0 iter 400: loss = 1.3503,  smooth loss = 1.4455
epoch 0 iter 450: loss = 1.3277,  smooth loss = 1.3930
epoch 0 iter 500: loss = 1.2789,  smooth loss = 1.3425
epoch 0 iter 550: loss = 1.2368,  smooth loss = 1.2984
epoch 0 iter 600: loss = 1.2348,  smooth loss = 1.2580
epoch 0 iter 650: loss = 1.2071,  smooth loss = 1.2188
epoch 0 iter 700: loss = 1.1686,  smooth loss = 1.1831
epoch 0 iter 750: loss = 1.1296,  smooth loss = 1.1544
epoch 0 iter 800: loss = 1.0890,  smooth loss = 1.1244
epoch 0 iter 850: loss = 1.0242,  smooth loss = 1.0927
epoch 0 iter 900: loss = 1.0104,  smooth loss = 1.0669
epoch 0 iter 950: loss = 1.0284,  smooth loss = 1.0455
epoch 0 iter 1000: loss = 0.9756,  smooth loss = 1.0185
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9634,  smooth loss = 0.9909
epoch 0 iter 1100: loss = 0.9807,  smooth loss = 0.9672
epoch 0 iter 1150: loss = 0.9576,  smooth loss = 0.9469
epoch 0 iter 1200: loss = 0.9269,  smooth loss = 0.9290
epoch 0 iter 1250: loss = 0.8997,  smooth loss = 0.9080
epoch 0 iter 1300: loss = 0.8932,  smooth loss = 0.8911
epoch 0 iter 1350: loss = 0.8530,  smooth loss = 0.8740
epoch 0 iter 1400: loss = 0.8385,  smooth loss = 0.8587
epoch 0 iter 1450: loss = 0.8019,  smooth loss = 0.8419
epoch 0 iter 1500: loss = 0.8017,  smooth loss = 0.8258
epoch 0 iter 1550: loss = 0.8023,  smooth loss = 0.8088
epoch 0 iter 1600: loss = 0.7883,  smooth loss = 0.7964
epoch 0 iter 1650: loss = 0.7831,  smooth loss = 0.7834
epoch 0 iter 1700: loss = 0.7443,  smooth loss = 0.7678
epoch 0 iter 1750: loss = 0.7539,  smooth loss = 0.7549
epoch 0 iter 1800: loss = 0.7184,  smooth loss = 0.7485
epoch 0 iter 1850: loss = 0.7277,  smooth loss = 0.7368
epoch 0 iter 1900: loss = 0.6938,  smooth loss = 0.7226
epoch 0 iter 1950: loss = 0.6603,  smooth loss = 0.7113
epoch 0 iter 2000: loss = 0.6873,  smooth loss = 0.7017
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.7390,  smooth loss = 0.6947
epoch 0 iter 2100: loss = 0.6844,  smooth loss = 0.6852
epoch 0 iter 2150: loss = 0.6874,  smooth loss = 0.6753
epoch 0 iter 2200: loss = 0.6858,  smooth loss = 0.6653
epoch 0 iter 2250: loss = 0.6391,  smooth loss = 0.6594
epoch 0 iter 2300: loss = 0.6338,  smooth loss = 0.6497
epoch 0 iter 2350: loss = 0.6202,  smooth loss = 0.6436
epoch 0 iter 2400: loss = 0.6403,  smooth loss = 0.6375
epoch 0 iter 2450: loss = 0.6120,  smooth loss = 0.6314
epoch 0 iter 2500: loss = 0.5951,  smooth loss = 0.6241
epoch 0 iter 2550: loss = 0.5844,  smooth loss = 0.6177
epoch 0 iter 2600: loss = 0.5877,  smooth loss = 0.6096
epoch 0 iter 2650: loss = 0.6156,  smooth loss = 0.6041
epoch 0 iter 2700: loss = 0.5913,  smooth loss = 0.5972
epoch 0 iter 2750: loss = 0.6151,  smooth loss = 0.5941
epoch 0 iter 2800: loss = 0.5550,  smooth loss = 0.5896
epoch 0 iter 2850: loss = 0.5721,  smooth loss = 0.5803
epoch 0 iter 2900: loss = 0.5799,  smooth loss = 0.5790
epoch 0 iter 2950: loss = 0.5609,  smooth loss = 0.5736
epoch 0 iter 3000: loss = 0.5881,  smooth loss = 0.5714
average data time = 0.0088s, average running time = 0.5367s
epoch 0 iter 3000: eval loss = 1.8050,  ccr = 0.8362,  cwr = 0.4854,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4854.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5787,  smooth loss = 0.5665
epoch 0 iter 3100: loss = 0.5377,  smooth loss = 0.5604
epoch 0 iter 3150: loss = 0.5280,  smooth loss = 0.5538
epoch 0 iter 3200: loss = 0.5546,  smooth loss = 0.5509
epoch 0 iter 3250: loss = 0.5602,  smooth loss = 0.5479
epoch 0 iter 3300: loss = 0.5528,  smooth loss = 0.5416
epoch 0 iter 3350: loss = 0.5195,  smooth loss = 0.5408
epoch 0 iter 3400: loss = 0.5420,  smooth loss = 0.5326
epoch 0 iter 3450: loss = 0.5432,  smooth loss = 0.5264
epoch 0 iter 3500: loss = 0.5187,  smooth loss = 0.5262
epoch 0 iter 3550: loss = 0.5278,  smooth loss = 0.5238
epoch 0 iter 3600: loss = 0.4752,  smooth loss = 0.5177
epoch 0 iter 3650: loss = 0.4985,  smooth loss = 0.5141
epoch 0 iter 3700: loss = 0.4933,  smooth loss = 0.5092
epoch 0 iter 3750: loss = 0.5152,  smooth loss = 0.5078
epoch 0 iter 3800: loss = 0.5046,  smooth loss = 0.5088
epoch 0 iter 3850: loss = 0.5407,  smooth loss = 0.5063
epoch 0 iter 3900: loss = 0.4845,  smooth loss = 0.5000
epoch 0 iter 3950: loss = 0.4993,  smooth loss = 0.4989
epoch 0 iter 4000: loss = 0.5072,  smooth loss = 0.4959
Save model pretrain-language-model_0_4000
epoch 0 iter 4050: loss = 0.4631,  smooth loss = 0.4936
epoch 0 iter 4100: loss = 0.4426,  smooth loss = 0.4859
epoch 0 iter 4150: loss = 0.4929,  smooth loss = 0.4846
epoch 0 iter 4200: loss = 0.4717,  smooth loss = 0.4848
epoch 0 iter 4250: loss = 0.4922,  smooth loss = 0.4811
epoch 0 iter 4300: loss = 0.4895,  smooth loss = 0.4791
epoch 0 iter 4350: loss = 0.4763,  smooth loss = 0.4783
epoch 0 iter 4400: loss = 0.4790,  smooth loss = 0.4744
epoch 0 iter 4450: loss = 0.4527,  smooth loss = 0.4748
epoch 0 iter 4500: loss = 0.4808,  smooth loss = 0.4728
epoch 0 iter 4550: loss = 0.4505,  smooth loss = 0.4708
epoch 0 iter 4600: loss = 0.4867,  smooth loss = 0.4672
epoch 0 iter 4650: loss = 0.4534,  smooth loss = 0.4634
epoch 0 iter 4700: loss = 0.4581,  smooth loss = 0.4645
epoch 0 iter 4750: loss = 0.4546,  smooth loss = 0.4601
epoch 0 iter 4800: loss = 0.4431,  smooth loss = 0.4571
epoch 0 iter 4850: loss = 0.4692,  smooth loss = 0.4525
epoch 0 iter 4900: loss = 0.4733,  smooth loss = 0.4539
epoch 0 iter 4950: loss = 0.4392,  smooth loss = 0.4530
epoch 0 iter 5000: loss = 0.4292,  smooth loss = 0.4518
Save model pretrain-language-model_0_5000
epoch 0 iter 5050: loss = 0.4315,  smooth loss = 0.4495
epoch 0 iter 5100: loss = 0.4462,  smooth loss = 0.4456
epoch 0 iter 5150: loss = 0.4742,  smooth loss = 0.4444
epoch 0 iter 5200: loss = 0.4474,  smooth loss = 0.4432
epoch 0 iter 5250: loss = 0.4302,  smooth loss = 0.4407
epoch 0 iter 5300: loss = 0.4232,  smooth loss = 0.4404
epoch 0 iter 5350: loss = 0.4483,  smooth loss = 0.4366
epoch 0 iter 5400: loss = 0.4226,  smooth loss = 0.4362
epoch 0 iter 5450: loss = 0.4477,  smooth loss = 0.4345
epoch 0 iter 5500: loss = 0.4169,  smooth loss = 0.4343
epoch 0 iter 5550: loss = 0.4461,  smooth loss = 0.4313
epoch 0 iter 5600: loss = 0.4363,  smooth loss = 0.4319
epoch 0 iter 5650: loss = 0.3865,  smooth loss = 0.4279
epoch 0 iter 5700: loss = 0.4222,  smooth loss = 0.4273
epoch 0 iter 5750: loss = 0.4454,  smooth loss = 0.4250
epoch 0 iter 5800: loss = 0.4285,  smooth loss = 0.4262
epoch 0 iter 5850: loss = 0.4470,  smooth loss = 0.4251
epoch 0 iter 5900: loss = 0.4176,  smooth loss = 0.4231
epoch 0 iter 5950: loss = 0.4153,  smooth loss = 0.4213
epoch 0 iter 6000: loss = 0.4183,  smooth loss = 0.4173
average data time = 0.0054s, average running time = 0.5422s
epoch 0 iter 6000: eval loss = 1.8874,  ccr = 0.8382,  cwr = 0.4924,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.4924.
Save model pretrain-language-model_0_6000
epoch 0 iter 6050: loss = 0.4219,  smooth loss = 0.4184
epoch 0 iter 6100: loss = 0.4238,  smooth loss = 0.4157
epoch 0 iter 6150: loss = 0.4283,  smooth loss = 0.4163
epoch 0 iter 6200: loss = 0.4164,  smooth loss = 0.4133
epoch 0 iter 6250: loss = 0.4273,  smooth loss = 0.4115
epoch 0 iter 6300: loss = 0.3897,  smooth loss = 0.4110
epoch 0 iter 6350: loss = 0.4414,  smooth loss = 0.4096
epoch 0 iter 6400: loss = 0.3953,  smooth loss = 0.4090
epoch 0 iter 6450: loss = 0.4099,  smooth loss = 0.4099
epoch 0 iter 6500: loss = 0.3769,  smooth loss = 0.4061
epoch 0 iter 6550: loss = 0.3928,  smooth loss = 0.4029
epoch 0 iter 6600: loss = 0.4010,  smooth loss = 0.4043
epoch 0 iter 6650: loss = 0.3800,  smooth loss = 0.4023
epoch 0 iter 6700: loss = 0.4139,  smooth loss = 0.4043
epoch 0 iter 6750: loss = 0.4257,  smooth loss = 0.4008
epoch 0 iter 6800: loss = 0.3949,  smooth loss = 0.3988
epoch 0 iter 6850: loss = 0.4192,  smooth loss = 0.3965
epoch 0 iter 6900: loss = 0.3911,  smooth loss = 0.3970
epoch 0 iter 6950: loss = 0.4036,  smooth loss = 0.3965
epoch 0 iter 7000: loss = 0.3712,  smooth loss = 0.3931
Save model pretrain-language-model_0_7000
epoch 0 iter 7050: loss = 0.4139,  smooth loss = 0.3939
epoch 0 iter 7100: loss = 0.3942,  smooth loss = 0.3918
epoch 0 iter 7150: loss = 0.3870,  smooth loss = 0.3888
epoch 0 iter 7200: loss = 0.3885,  smooth loss = 0.3905
epoch 0 iter 7250: loss = 0.3948,  smooth loss = 0.3912
epoch 0 iter 7300: loss = 0.3942,  smooth loss = 0.3886
epoch 0 iter 7350: loss = 0.3650,  smooth loss = 0.3878
epoch 0 iter 7400: loss = 0.3949,  smooth loss = 0.3879
epoch 0 iter 7450: loss = 0.3848,  smooth loss = 0.3837
epoch 0 iter 7500: loss = 0.3801,  smooth loss = 0.3826
epoch 0 iter 7550: loss = 0.3980,  smooth loss = 0.3844
epoch 0 iter 7600: loss = 0.3702,  smooth loss = 0.3859
epoch 0 iter 7650: loss = 0.3999,  smooth loss = 0.3863
epoch 0 iter 7700: loss = 0.3930,  smooth loss = 0.3850
epoch 0 iter 7750: loss = 0.3883,  smooth loss = 0.3832
epoch 0 iter 7800: loss = 0.3695,  smooth loss = 0.3805
epoch 0 iter 7850: loss = 0.3904,  smooth loss = 0.3801
epoch 0 iter 7900: loss = 0.3833,  smooth loss = 0.3783
epoch 0 iter 7950: loss = 0.4035,  smooth loss = 0.3780
epoch 0 iter 8000: loss = 0.3780,  smooth loss = 0.3776
Save model pretrain-language-model_0_8000
epoch 0 iter 8050: loss = 0.3626,  smooth loss = 0.3785
epoch 0 iter 8100: loss = 0.3663,  smooth loss = 0.3765
epoch 0 iter 8150: loss = 0.3591,  smooth loss = 0.3747
epoch 0 iter 8200: loss = 0.3527,  smooth loss = 0.3769
epoch 0 iter 8250: loss = 0.3944,  smooth loss = 0.3731
epoch 0 iter 8300: loss = 0.3747,  smooth loss = 0.3723
epoch 0 iter 8350: loss = 0.3899,  smooth loss = 0.3707
epoch 0 iter 8400: loss = 0.3785,  smooth loss = 0.3704
epoch 0 iter 8450: loss = 0.3703,  smooth loss = 0.3694
epoch 0 iter 8500: loss = 0.3748,  smooth loss = 0.3688
epoch 0 iter 8550: loss = 0.3704,  smooth loss = 0.3693
epoch 0 iter 8600: loss = 0.3573,  smooth loss = 0.3718
epoch 0 iter 8650: loss = 0.3607,  smooth loss = 0.3709
epoch 0 iter 8700: loss = 0.3579,  smooth loss = 0.3676
epoch 0 iter 8750: loss = 0.3545,  smooth loss = 0.3651
epoch 0 iter 8800: loss = 0.3690,  smooth loss = 0.3666
epoch 0 iter 8850: loss = 0.3537,  smooth loss = 0.3688
epoch 0 iter 8900: loss = 0.3686,  smooth loss = 0.3683
epoch 0 iter 8950: loss = 0.3576,  smooth loss = 0.3652
epoch 0 iter 9000: loss = 0.3631,  smooth loss = 0.3641
average data time = 0.0044s, average running time = 0.5448s
epoch 0 iter 9000: eval loss = 1.9369,  ccr = 0.8378,  cwr = 0.4919,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_9000
epoch 0 iter 9050: loss = 0.3831,  smooth loss = 0.3641
epoch 0 iter 9100: loss = 0.3650,  smooth loss = 0.3619
epoch 0 iter 9150: loss = 0.3448,  smooth loss = 0.3593
epoch 0 iter 9200: loss = 0.3766,  smooth loss = 0.3575
epoch 0 iter 9250: loss = 0.3704,  smooth loss = 0.3588
epoch 0 iter 9300: loss = 0.3305,  smooth loss = 0.3579
epoch 0 iter 9350: loss = 0.3323,  smooth loss = 0.3567
epoch 0 iter 9400: loss = 0.3532,  smooth loss = 0.3556
epoch 0 iter 9450: loss = 0.3590,  smooth loss = 0.3575
epoch 0 iter 9500: loss = 0.3407,  smooth loss = 0.3576
epoch 0 iter 9550: loss = 0.3687,  smooth loss = 0.3565
epoch 0 iter 9600: loss = 0.3506,  smooth loss = 0.3575
epoch 0 iter 9650: loss = 0.3254,  smooth loss = 0.3568
epoch 0 iter 9700: loss = 0.3817,  smooth loss = 0.3578
epoch 0 iter 9750: loss = 0.3726,  smooth loss = 0.3544
epoch 0 iter 9800: loss = 0.3642,  smooth loss = 0.3547
epoch 0 iter 9850: loss = 0.3212,  smooth loss = 0.3523
epoch 0 iter 9900: loss = 0.3531,  smooth loss = 0.3530
epoch 0 iter 9950: loss = 0.3461,  smooth loss = 0.3509
epoch 0 iter 10000: loss = 0.3719,  smooth loss = 0.3508
Save model pretrain-language-model_0_10000
epoch 0 iter 10050: loss = 0.3504,  smooth loss = 0.3532
epoch 0 iter 10100: loss = 0.3483,  smooth loss = 0.3516
epoch 0 iter 10150: loss = 0.3498,  smooth loss = 0.3513
epoch 0 iter 10200: loss = 0.3346,  smooth loss = 0.3484
epoch 0 iter 10250: loss = 0.3646,  smooth loss = 0.3482
epoch 0 iter 10300: loss = 0.3528,  smooth loss = 0.3471
epoch 0 iter 10350: loss = 0.3505,  smooth loss = 0.3471
epoch 0 iter 10400: loss = 0.3569,  smooth loss = 0.3459
epoch 0 iter 10450: loss = 0.3403,  smooth loss = 0.3445
epoch 0 iter 10500: loss = 0.3457,  smooth loss = 0.3437
epoch 0 iter 10550: loss = 0.3353,  smooth loss = 0.3462
epoch 0 iter 10600: loss = 0.3749,  smooth loss = 0.3466
epoch 0 iter 10650: loss = 0.3655,  smooth loss = 0.3449
epoch 0 iter 10700: loss = 0.3339,  smooth loss = 0.3428
epoch 0 iter 10750: loss = 0.3382,  smooth loss = 0.3431
epoch 0 iter 10800: loss = 0.3343,  smooth loss = 0.3440
epoch 0 iter 10850: loss = 0.3481,  smooth loss = 0.3414
epoch 0 iter 10900: loss = 0.3430,  smooth loss = 0.3421
epoch 0 iter 10950: loss = 0.3478,  smooth loss = 0.3401
epoch 0 iter 11000: loss = 0.3340,  smooth loss = 0.3418
Save model pretrain-language-model_0_11000
epoch 0 iter 11050: loss = 0.3580,  smooth loss = 0.3440
epoch 0 iter 11100: loss = 0.3344,  smooth loss = 0.3406
epoch 0 iter 11150: loss = 0.3130,  smooth loss = 0.3425
epoch 0 iter 11200: loss = 0.3465,  smooth loss = 0.3395
epoch 0 iter 11250: loss = 0.3514,  smooth loss = 0.3392
epoch 0 iter 11300: loss = 0.3388,  smooth loss = 0.3392
epoch 0 iter 11350: loss = 0.3412,  smooth loss = 0.3382
epoch 0 iter 11400: loss = 0.3685,  smooth loss = 0.3374
epoch 0 iter 11450: loss = 0.3766,  smooth loss = 0.3392
epoch 0 iter 11500: loss = 0.3131,  smooth loss = 0.3363
epoch 0 iter 11550: loss = 0.3499,  smooth loss = 0.3372
epoch 0 iter 11600: loss = 0.3579,  smooth loss = 0.3363
epoch 0 iter 11650: loss = 0.3436,  smooth loss = 0.3375
epoch 0 iter 11700: loss = 0.3723,  smooth loss = 0.3376
epoch 0 iter 11750: loss = 0.3279,  smooth loss = 0.3349
epoch 0 iter 11800: loss = 0.3302,  smooth loss = 0.3339
epoch 0 iter 11850: loss = 0.3251,  smooth loss = 0.3348
epoch 0 iter 11900: loss = 0.3340,  smooth loss = 0.3315
epoch 0 iter 11950: loss = 0.3438,  smooth loss = 0.3316
epoch 0 iter 12000: loss = 0.3380,  smooth loss = 0.3350
average data time = 0.0038s, average running time = 0.5462s
epoch 0 iter 12000: eval loss = 1.9808,  ccr = 0.8384,  cwr = 0.4910,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_12000
epoch 0 iter 12050: loss = 0.3181,  smooth loss = 0.3315
epoch 0 iter 12100: loss = 0.3578,  smooth loss = 0.3321
epoch 0 iter 12150: loss = 0.3310,  smooth loss = 0.3318
epoch 0 iter 12200: loss = 0.3208,  smooth loss = 0.3318
epoch 0 iter 12250: loss = 0.3519,  smooth loss = 0.3296
epoch 0 iter 12300: loss = 0.3630,  smooth loss = 0.3287
epoch 0 iter 12350: loss = 0.3222,  smooth loss = 0.3297
epoch 0 iter 12400: loss = 0.3415,  smooth loss = 0.3301
epoch 0 iter 12450: loss = 0.3124,  smooth loss = 0.3298
epoch 0 iter 12500: loss = 0.3431,  smooth loss = 0.3295
epoch 0 iter 12550: loss = 0.3049,  smooth loss = 0.3272
epoch 0 iter 12600: loss = 0.3228,  smooth loss = 0.3274
epoch 0 iter 12650: loss = 0.3258,  smooth loss = 0.3263
epoch 0 iter 12700: loss = 0.3330,  smooth loss = 0.3256
epoch 0 iter 12750: loss = 0.3201,  smooth loss = 0.3261
epoch 0 iter 12800: loss = 0.3334,  smooth loss = 0.3260
epoch 0 iter 12850: loss = 0.3365,  smooth loss = 0.3244
epoch 0 iter 12900: loss = 0.3181,  smooth loss = 0.3240
epoch 0 iter 12950: loss = 0.3291,  smooth loss = 0.3254
epoch 0 iter 13000: loss = 0.2986,  smooth loss = 0.3251
Save model pretrain-language-model_0_13000
epoch 0 iter 13050: loss = 0.3293,  smooth loss = 0.3241
epoch 0 iter 13100: loss = 0.3149,  smooth loss = 0.3221
epoch 0 iter 13150: loss = 0.3337,  smooth loss = 0.3218
epoch 0 iter 13200: loss = 0.3180,  smooth loss = 0.3201
epoch 0 iter 13250: loss = 0.3019,  smooth loss = 0.3224
epoch 0 iter 13300: loss = 0.3124,  smooth loss = 0.3206
epoch 0 iter 13350: loss = 0.3336,  smooth loss = 0.3222
epoch 0 iter 13400: loss = 0.3327,  smooth loss = 0.3238
epoch 0 iter 13450: loss = 0.3154,  smooth loss = 0.3219
epoch 0 iter 13500: loss = 0.3089,  smooth loss = 0.3212
epoch 0 iter 13550: loss = 0.3298,  smooth loss = 0.3206
epoch 0 iter 13600: loss = 0.3211,  smooth loss = 0.3201
epoch 0 iter 13650: loss = 0.3022,  smooth loss = 0.3196
epoch 0 iter 13700: loss = 0.3209,  smooth loss = 0.3178
epoch 0 iter 13750: loss = 0.2921,  smooth loss = 0.3189
epoch 0 iter 13800: loss = 0.3448,  smooth loss = 0.3205
epoch 0 iter 13850: loss = 0.3430,  smooth loss = 0.3213
epoch 0 iter 13900: loss = 0.3212,  smooth loss = 0.3194
epoch 0 iter 13950: loss = 0.3145,  smooth loss = 0.3190
epoch 0 iter 14000: loss = 0.3141,  smooth loss = 0.3185
Save model pretrain-language-model_0_14000
epoch 0 iter 14050: loss = 0.3188,  smooth loss = 0.3169
epoch 0 iter 14100: loss = 0.3224,  smooth loss = 0.3191
epoch 0 iter 14150: loss = 0.3313,  smooth loss = 0.3174
epoch 0 iter 14200: loss = 0.3111,  smooth loss = 0.3151
epoch 0 iter 14250: loss = 0.3004,  smooth loss = 0.3167
epoch 0 iter 14300: loss = 0.3391,  smooth loss = 0.3179
epoch 0 iter 14350: loss = 0.3330,  smooth loss = 0.3174
epoch 0 iter 14400: loss = 0.3251,  smooth loss = 0.3171
epoch 0 iter 14450: loss = 0.3374,  smooth loss = 0.3178
epoch 0 iter 14500: loss = 0.3180,  smooth loss = 0.3151
epoch 0 iter 14550: loss = 0.3118,  smooth loss = 0.3170
epoch 0 iter 14600: loss = 0.3059,  smooth loss = 0.3152
epoch 0 iter 14650: loss = 0.3411,  smooth loss = 0.3149
epoch 0 iter 14700: loss = 0.3298,  smooth loss = 0.3140
epoch 0 iter 14750: loss = 0.3169,  smooth loss = 0.3153
epoch 0 iter 14800: loss = 0.3291,  smooth loss = 0.3131
epoch 0 iter 14850: loss = 0.3280,  smooth loss = 0.3108
epoch 0 iter 14900: loss = 0.2889,  smooth loss = 0.3116
epoch 0 iter 14950: loss = 0.3050,  smooth loss = 0.3104
epoch 0 iter 15000: loss = 0.3256,  smooth loss = 0.3119
average data time = 0.0035s, average running time = 0.5467s
epoch 0 iter 15000: eval loss = 2.0230,  ccr = 0.8372,  cwr = 0.4888,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_15000
epoch 0 iter 15050: loss = 0.2944,  smooth loss = 0.3092
epoch 0 iter 15100: loss = 0.3388,  smooth loss = 0.3129
epoch 0 iter 15150: loss = 0.3083,  smooth loss = 0.3109
epoch 0 iter 15200: loss = 0.3148,  smooth loss = 0.3136
epoch 0 iter 15250: loss = 0.3228,  smooth loss = 0.3132
epoch 0 iter 15300: loss = 0.3103,  smooth loss = 0.3102
epoch 0 iter 15350: loss = 0.2979,  smooth loss = 0.3106
epoch 0 iter 15400: loss = 0.3121,  smooth loss = 0.3095
epoch 0 iter 15450: loss = 0.3099,  smooth loss = 0.3107
epoch 0 iter 15500: loss = 0.3019,  smooth loss = 0.3102
epoch 0 iter 15550: loss = 0.3439,  smooth loss = 0.3112
epoch 0 iter 15600: loss = 0.2944,  smooth loss = 0.3123
epoch 0 iter 15650: loss = 0.3058,  smooth loss = 0.3116
epoch 0 iter 15700: loss = 0.2886,  smooth loss = 0.3117
epoch 0 iter 15750: loss = 0.3209,  smooth loss = 0.3119
epoch 0 iter 15800: loss = 0.2989,  smooth loss = 0.3098
epoch 0 iter 15850: loss = 0.3234,  smooth loss = 0.3112
epoch 0 iter 15900: loss = 0.3211,  smooth loss = 0.3104
epoch 0 iter 15950: loss = 0.3155,  smooth loss = 0.3092
epoch 0 iter 16000: loss = 0.3093,  smooth loss = 0.3085
Save model pretrain-language-model_0_16000
epoch 0 iter 16050: loss = 0.2982,  smooth loss = 0.3070
epoch 0 iter 16100: loss = 0.2937,  smooth loss = 0.3081
epoch 0 iter 16150: loss = 0.3039,  smooth loss = 0.3061
epoch 0 iter 16200: loss = 0.3267,  smooth loss = 0.3073
epoch 0 iter 16250: loss = 0.3280,  smooth loss = 0.3070
epoch 0 iter 16300: loss = 0.3294,  smooth loss = 0.3076
epoch 0 iter 16350: loss = 0.3177,  smooth loss = 0.3061
epoch 0 iter 16400: loss = 0.3024,  smooth loss = 0.3060
epoch 0 iter 16450: loss = 0.3201,  smooth loss = 0.3057
epoch 0 iter 16500: loss = 0.2944,  smooth loss = 0.3051
epoch 0 iter 16550: loss = 0.2998,  smooth loss = 0.3052
epoch 0 iter 16600: loss = 0.3321,  smooth loss = 0.3063
epoch 0 iter 16650: loss = 0.3088,  smooth loss = 0.3058
epoch 0 iter 16700: loss = 0.2865,  smooth loss = 0.3043
epoch 0 iter 16750: loss = 0.3003,  smooth loss = 0.3037
epoch 0 iter 16800: loss = 0.3203,  smooth loss = 0.3037
epoch 0 iter 16850: loss = 0.2999,  smooth loss = 0.3031
epoch 0 iter 16900: loss = 0.2930,  smooth loss = 0.3039
epoch 0 iter 16950: loss = 0.2967,  smooth loss = 0.3039
epoch 0 iter 17000: loss = 0.3004,  smooth loss = 0.3049
Save model pretrain-language-model_0_17000
epoch 0 iter 17050: loss = 0.2879,  smooth loss = 0.3034
epoch 0 iter 17100: loss = 0.2752,  smooth loss = 0.3019
epoch 0 iter 17150: loss = 0.3034,  smooth loss = 0.3020
epoch 0 iter 17200: loss = 0.2916,  smooth loss = 0.3047
epoch 0 iter 17250: loss = 0.3061,  smooth loss = 0.2997
epoch 0 iter 17300: loss = 0.2730,  smooth loss = 0.3003
epoch 0 iter 17350: loss = 0.3087,  smooth loss = 0.3018
epoch 0 iter 17400: loss = 0.3054,  smooth loss = 0.3037
epoch 0 iter 17450: loss = 0.3036,  smooth loss = 0.3035
epoch 0 iter 17500: loss = 0.3080,  smooth loss = 0.3039
epoch 0 iter 17550: loss = 0.2898,  smooth loss = 0.3008
epoch 0 iter 17600: loss = 0.2998,  smooth loss = 0.3015
epoch 0 iter 17650: loss = 0.3041,  smooth loss = 0.3011
epoch 0 iter 17700: loss = 0.3052,  smooth loss = 0.3027
epoch 0 iter 17750: loss = 0.2869,  smooth loss = 0.3024
epoch 0 iter 17800: loss = 0.2916,  smooth loss = 0.3019
epoch 0 iter 17850: loss = 0.2955,  smooth loss = 0.2999
epoch 0 iter 17900: loss = 0.2983,  smooth loss = 0.2998
epoch 0 iter 17950: loss = 0.3238,  smooth loss = 0.3011
epoch 0 iter 18000: loss = 0.2997,  smooth loss = 0.3014
average data time = 0.0033s, average running time = 0.5473s
epoch 0 iter 18000: eval loss = 2.0309,  ccr = 0.8380,  cwr = 0.4943,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 18000 with accuracy value: 0.4943.
Save model pretrain-language-model_0_18000
epoch 0 iter 18050: loss = 0.2950,  smooth loss = 0.3010
epoch 0 iter 18100: loss = 0.3104,  smooth loss = 0.3003
epoch 0 iter 18150: loss = 0.2941,  smooth loss = 0.3009
epoch 0 iter 18200: loss = 0.2996,  smooth loss = 0.2984
epoch 0 iter 18250: loss = 0.3059,  smooth loss = 0.2982
epoch 0 iter 18300: loss = 0.3047,  smooth loss = 0.3000
epoch 0 iter 18350: loss = 0.2870,  smooth loss = 0.2983
epoch 0 iter 18400: loss = 0.2894,  smooth loss = 0.2985
epoch 0 iter 18450: loss = 0.2895,  smooth loss = 0.3003
epoch 0 iter 18500: loss = 0.2948,  smooth loss = 0.2996
epoch 0 iter 18550: loss = 0.3082,  smooth loss = 0.2960
epoch 0 iter 18600: loss = 0.2989,  smooth loss = 0.2944
epoch 0 iter 18650: loss = 0.3114,  smooth loss = 0.2967
epoch 0 iter 18700: loss = 0.2979,  smooth loss = 0.2973
epoch 0 iter 18750: loss = 0.3197,  smooth loss = 0.2952
epoch 0 iter 18800: loss = 0.2958,  smooth loss = 0.2957
epoch 0 iter 18850: loss = 0.2993,  smooth loss = 0.2980
epoch 0 iter 18900: loss = 0.3223,  smooth loss = 0.2948
epoch 0 iter 18950: loss = 0.3213,  smooth loss = 0.2962
epoch 0 iter 19000: loss = 0.3030,  smooth loss = 0.2962
Save model pretrain-language-model_0_19000
epoch 0 iter 19050: loss = 0.2785,  smooth loss = 0.2962
epoch 0 iter 19100: loss = 0.3072,  smooth loss = 0.2936
epoch 0 iter 19150: loss = 0.3118,  smooth loss = 0.2937
epoch 0 iter 19200: loss = 0.2844,  smooth loss = 0.2939
epoch 0 iter 19250: loss = 0.3003,  smooth loss = 0.2940
epoch 0 iter 19300: loss = 0.3018,  smooth loss = 0.2958
epoch 0 iter 19350: loss = 0.2928,  smooth loss = 0.2938
epoch 0 iter 19400: loss = 0.2945,  smooth loss = 0.2928
epoch 0 iter 19450: loss = 0.3015,  smooth loss = 0.2942
epoch 0 iter 19500: loss = 0.2903,  smooth loss = 0.2941
epoch 0 iter 19550: loss = 0.2999,  smooth loss = 0.2948
epoch 0 iter 19600: loss = 0.3237,  smooth loss = 0.2960
epoch 0 iter 19650: loss = 0.3022,  smooth loss = 0.2947
epoch 0 iter 19700: loss = 0.2599,  smooth loss = 0.2936
epoch 0 iter 19750: loss = 0.2888,  smooth loss = 0.2930
epoch 0 iter 19800: loss = 0.2872,  smooth loss = 0.2939
epoch 0 iter 19850: loss = 0.3165,  smooth loss = 0.2912
epoch 0 iter 19900: loss = 0.3027,  smooth loss = 0.2931
epoch 0 iter 19950: loss = 0.2944,  smooth loss = 0.2930
epoch 0 iter 20000: loss = 0.2990,  smooth loss = 0.2934
Save model pretrain-language-model_0_20000
epoch 0 iter 20050: loss = 0.2937,  smooth loss = 0.2934
epoch 0 iter 20100: loss = 0.3163,  smooth loss = 0.2941
epoch 0 iter 20150: loss = 0.2980,  smooth loss = 0.2941
epoch 0 iter 20200: loss = 0.2963,  smooth loss = 0.2931
epoch 0 iter 20250: loss = 0.2742,  smooth loss = 0.2931
epoch 0 iter 20300: loss = 0.2885,  smooth loss = 0.2937
epoch 0 iter 20350: loss = 0.2904,  smooth loss = 0.2919
epoch 0 iter 20400: loss = 0.2760,  smooth loss = 0.2907
epoch 0 iter 20450: loss = 0.2715,  smooth loss = 0.2919
epoch 0 iter 20500: loss = 0.2855,  smooth loss = 0.2902
epoch 0 iter 20550: loss = 0.2903,  smooth loss = 0.2876
epoch 0 iter 20600: loss = 0.2814,  smooth loss = 0.2910
epoch 0 iter 20650: loss = 0.2833,  smooth loss = 0.2920
epoch 0 iter 20700: loss = 0.2782,  smooth loss = 0.2872
epoch 0 iter 20750: loss = 0.2734,  smooth loss = 0.2885
epoch 0 iter 20800: loss = 0.2806,  smooth loss = 0.2881
epoch 0 iter 20850: loss = 0.2929,  smooth loss = 0.2878
epoch 0 iter 20900: loss = 0.2929,  smooth loss = 0.2888
epoch 0 iter 20950: loss = 0.2945,  smooth loss = 0.2909
epoch 0 iter 21000: loss = 0.2763,  smooth loss = 0.2886
average data time = 0.0031s, average running time = 0.5479s
epoch 0 iter 21000: eval loss = 2.0658,  ccr = 0.8392,  cwr = 0.4961,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 21000 with accuracy value: 0.4961.
Save model pretrain-language-model_0_21000
epoch 0 iter 21050: loss = 0.2972,  smooth loss = 0.2894
epoch 0 iter 21100: loss = 0.2852,  smooth loss = 0.2883
epoch 0 iter 21150: loss = 0.3284,  smooth loss = 0.2887
epoch 0 iter 21200: loss = 0.2905,  smooth loss = 0.2878
epoch 0 iter 21250: loss = 0.3007,  smooth loss = 0.2891
epoch 0 iter 21300: loss = 0.2944,  smooth loss = 0.2919
epoch 0 iter 21350: loss = 0.2824,  smooth loss = 0.2889
epoch 0 iter 21400: loss = 0.2697,  smooth loss = 0.2889
epoch 0 iter 21450: loss = 0.2805,  smooth loss = 0.2864
epoch 0 iter 21500: loss = 0.2687,  smooth loss = 0.2850
epoch 0 iter 21550: loss = 0.2718,  smooth loss = 0.2844
epoch 0 iter 21600: loss = 0.2709,  smooth loss = 0.2855
epoch 0 iter 21650: loss = 0.2654,  smooth loss = 0.2880
epoch 0 iter 21700: loss = 0.2972,  smooth loss = 0.2888
epoch 0 iter 21750: loss = 0.2827,  smooth loss = 0.2875
epoch 0 iter 21800: loss = 0.2806,  smooth loss = 0.2864
epoch 0 iter 21850: loss = 0.2818,  smooth loss = 0.2865
epoch 0 iter 21900: loss = 0.2815,  smooth loss = 0.2853
epoch 0 iter 21950: loss = 0.2753,  smooth loss = 0.2854
epoch 0 iter 22000: loss = 0.2847,  smooth loss = 0.2866
Save model pretrain-language-model_0_22000
epoch 0 iter 22050: loss = 0.2678,  smooth loss = 0.2855
epoch 0 iter 22100: loss = 0.3235,  smooth loss = 0.2875
epoch 0 iter 22150: loss = 0.2647,  smooth loss = 0.2845
epoch 0 iter 22200: loss = 0.3012,  smooth loss = 0.2846
epoch 0 iter 22250: loss = 0.2764,  smooth loss = 0.2845
epoch 0 iter 22300: loss = 0.2905,  smooth loss = 0.2852
epoch 0 iter 22350: loss = 0.2611,  smooth loss = 0.2863
epoch 0 iter 22400: loss = 0.2779,  smooth loss = 0.2849
epoch 0 iter 22450: loss = 0.2745,  smooth loss = 0.2832
epoch 0 iter 22500: loss = 0.2902,  smooth loss = 0.2837
epoch 0 iter 22550: loss = 0.2858,  smooth loss = 0.2846
epoch 0 iter 22600: loss = 0.2926,  smooth loss = 0.2853
epoch 0 iter 22650: loss = 0.2974,  smooth loss = 0.2864
epoch 0 iter 22700: loss = 0.2725,  smooth loss = 0.2847
epoch 0 iter 22750: loss = 0.2633,  smooth loss = 0.2852
epoch 0 iter 22800: loss = 0.2698,  smooth loss = 0.2853
epoch 0 iter 22850: loss = 0.2953,  smooth loss = 0.2863
epoch 0 iter 22900: loss = 0.2903,  smooth loss = 0.2850
epoch 0 iter 22950: loss = 0.2818,  smooth loss = 0.2842
epoch 0 iter 23000: loss = 0.2916,  smooth loss = 0.2820
Save model pretrain-language-model_0_23000
epoch 0 iter 23050: loss = 0.2939,  smooth loss = 0.2846
epoch 0 iter 23100: loss = 0.2638,  smooth loss = 0.2830
epoch 0 iter 23150: loss = 0.2844,  smooth loss = 0.2822
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Use 2 GPUs.
Start training.
epoch 0 iter 50: loss = 2.2471,  smooth loss = 2.4559
epoch 0 iter 100: loss = 2.0067,  smooth loss = 2.2113
epoch 0 iter 150: loss = 1.7898,  smooth loss = 1.9906
epoch 0 iter 200: loss = 1.6652,  smooth loss = 1.8093
epoch 0 iter 250: loss = 1.5796,  smooth loss = 1.6838
epoch 0 iter 300: loss = 1.5362,  smooth loss = 1.5861
epoch 0 iter 350: loss = 1.4758,  smooth loss = 1.5127
epoch 0 iter 400: loss = 1.3697,  smooth loss = 1.4475
epoch 0 iter 450: loss = 1.3380,  smooth loss = 1.3913
epoch 0 iter 500: loss = 1.2780,  smooth loss = 1.3393
epoch 0 iter 550: loss = 1.2551,  smooth loss = 1.2940
epoch 0 iter 600: loss = 1.2600,  smooth loss = 1.2541
epoch 0 iter 650: loss = 1.1689,  smooth loss = 1.2202
epoch 0 iter 700: loss = 1.1704,  smooth loss = 1.1883
epoch 0 iter 750: loss = 1.1011,  smooth loss = 1.1537
epoch 0 iter 800: loss = 1.0917,  smooth loss = 1.1227
epoch 0 iter 850: loss = 1.0477,  smooth loss = 1.0937
epoch 0 iter 900: loss = 1.0414,  smooth loss = 1.0676
epoch 0 iter 950: loss = 1.0343,  smooth loss = 1.0411
epoch 0 iter 1000: loss = 0.9622,  smooth loss = 1.0151
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9885,  smooth loss = 0.9934
epoch 0 iter 1100: loss = 0.9903,  smooth loss = 0.9733
epoch 0 iter 1150: loss = 0.9328,  smooth loss = 0.9520
epoch 0 iter 1200: loss = 0.9190,  smooth loss = 0.9303
epoch 0 iter 1250: loss = 0.8675,  smooth loss = 0.9115
epoch 0 iter 1300: loss = 0.8910,  smooth loss = 0.8903
epoch 0 iter 1350: loss = 0.8842,  smooth loss = 0.8719
epoch 0 iter 1400: loss = 0.8575,  smooth loss = 0.8565
epoch 0 iter 1450: loss = 0.8209,  smooth loss = 0.8394
epoch 0 iter 1500: loss = 0.7884,  smooth loss = 0.8248
epoch 0 iter 1550: loss = 0.8019,  smooth loss = 0.8084
epoch 0 iter 1600: loss = 0.7969,  smooth loss = 0.7959
epoch 0 iter 1650: loss = 0.7251,  smooth loss = 0.7845
epoch 0 iter 1700: loss = 0.7339,  smooth loss = 0.7714
epoch 0 iter 1750: loss = 0.7136,  smooth loss = 0.7595
epoch 0 iter 1800: loss = 0.7552,  smooth loss = 0.7494
epoch 0 iter 1850: loss = 0.7161,  smooth loss = 0.7363
epoch 0 iter 1900: loss = 0.7194,  smooth loss = 0.7246
epoch 0 iter 1950: loss = 0.7372,  smooth loss = 0.7145
epoch 0 iter 2000: loss = 0.7035,  smooth loss = 0.7064
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.6938,  smooth loss = 0.6971
epoch 0 iter 2100: loss = 0.6790,  smooth loss = 0.6890
epoch 0 iter 2150: loss = 0.6585,  smooth loss = 0.6800
epoch 0 iter 2200: loss = 0.6456,  smooth loss = 0.6697
epoch 0 iter 2250: loss = 0.6310,  smooth loss = 0.6616
epoch 0 iter 2300: loss = 0.6775,  smooth loss = 0.6557
epoch 0 iter 2350: loss = 0.6515,  smooth loss = 0.6467
epoch 0 iter 2400: loss = 0.6380,  smooth loss = 0.6408
epoch 0 iter 2450: loss = 0.6437,  smooth loss = 0.6346
epoch 0 iter 2500: loss = 0.6755,  smooth loss = 0.6289
epoch 0 iter 2550: loss = 0.6164,  smooth loss = 0.6221
epoch 0 iter 2600: loss = 0.6097,  smooth loss = 0.6121
epoch 0 iter 2650: loss = 0.5999,  smooth loss = 0.6072
epoch 0 iter 2700: loss = 0.5750,  smooth loss = 0.6041
epoch 0 iter 2750: loss = 0.5879,  smooth loss = 0.5981
epoch 0 iter 2800: loss = 0.5684,  smooth loss = 0.5915
epoch 0 iter 2850: loss = 0.5729,  smooth loss = 0.5864
epoch 0 iter 2900: loss = 0.5919,  smooth loss = 0.5814
epoch 0 iter 2950: loss = 0.6093,  smooth loss = 0.5749
epoch 0 iter 3000: loss = 0.5639,  smooth loss = 0.5693
average data time = 0.0088s, average running time = 0.5326s
epoch 0 iter 3000: eval loss = 1.8100,  ccr = 0.8363,  cwr = 0.4851,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4851.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5545,  smooth loss = 0.5664
epoch 0 iter 3100: loss = 0.5633,  smooth loss = 0.5642
epoch 0 iter 3150: loss = 0.5657,  smooth loss = 0.5598
epoch 0 iter 3200: loss = 0.5341,  smooth loss = 0.5545
epoch 0 iter 3250: loss = 0.5496,  smooth loss = 0.5496
epoch 0 iter 3300: loss = 0.5831,  smooth loss = 0.5474
epoch 0 iter 3350: loss = 0.5662,  smooth loss = 0.5442
epoch 0 iter 3400: loss = 0.5695,  smooth loss = 0.5388
epoch 0 iter 3450: loss = 0.5453,  smooth loss = 0.5344
epoch 0 iter 3500: loss = 0.5196,  smooth loss = 0.5292
epoch 0 iter 3550: loss = 0.5331,  smooth loss = 0.5262
epoch 0 iter 3600: loss = 0.4992,  smooth loss = 0.5233
epoch 0 iter 3650: loss = 0.5496,  smooth loss = 0.5223
epoch 0 iter 3700: loss = 0.5170,  smooth loss = 0.5170
epoch 0 iter 3750: loss = 0.5159,  smooth loss = 0.5122
epoch 0 iter 3800: loss = 0.4905,  smooth loss = 0.5078
epoch 0 iter 3850: loss = 0.5254,  smooth loss = 0.5066
epoch 0 iter 3900: loss = 0.5135,  smooth loss = 0.5050
epoch 0 iter 3950: loss = 0.5027,  smooth loss = 0.4997
epoch 0 iter 4000: loss = 0.4970,  smooth loss = 0.4994
Save model pretrain-language-model_0_4000
epoch 0 iter 4050: loss = 0.4906,  smooth loss = 0.4961
epoch 0 iter 4100: loss = 0.4780,  smooth loss = 0.4932
epoch 0 iter 4150: loss = 0.4928,  smooth loss = 0.4913
epoch 0 iter 4200: loss = 0.4919,  smooth loss = 0.4903
epoch 0 iter 4250: loss = 0.4957,  smooth loss = 0.4883
epoch 0 iter 4300: loss = 0.4881,  smooth loss = 0.4811
epoch 0 iter 4350: loss = 0.4744,  smooth loss = 0.4817
epoch 0 iter 4400: loss = 0.4782,  smooth loss = 0.4756
epoch 0 iter 4450: loss = 0.4821,  smooth loss = 0.4756
epoch 0 iter 4500: loss = 0.4672,  smooth loss = 0.4745
epoch 0 iter 4550: loss = 0.4718,  smooth loss = 0.4726
epoch 0 iter 4600: loss = 0.4468,  smooth loss = 0.4703
epoch 0 iter 4650: loss = 0.4652,  smooth loss = 0.4665
epoch 0 iter 4700: loss = 0.4691,  smooth loss = 0.4636
epoch 0 iter 4750: loss = 0.4488,  smooth loss = 0.4620
epoch 0 iter 4800: loss = 0.4649,  smooth loss = 0.4622
epoch 0 iter 4850: loss = 0.4556,  smooth loss = 0.4562
epoch 0 iter 4900: loss = 0.4367,  smooth loss = 0.4560
epoch 0 iter 4950: loss = 0.4614,  smooth loss = 0.4553
epoch 0 iter 5000: loss = 0.4519,  smooth loss = 0.4524
Save model pretrain-language-model_0_5000
epoch 0 iter 5050: loss = 0.4320,  smooth loss = 0.4520
epoch 0 iter 5100: loss = 0.4523,  smooth loss = 0.4497
epoch 0 iter 5150: loss = 0.4584,  smooth loss = 0.4466
epoch 0 iter 5200: loss = 0.4259,  smooth loss = 0.4459
epoch 0 iter 5250: loss = 0.4371,  smooth loss = 0.4431
epoch 0 iter 5300: loss = 0.4579,  smooth loss = 0.4412
epoch 0 iter 5350: loss = 0.4491,  smooth loss = 0.4434
epoch 0 iter 5400: loss = 0.4247,  smooth loss = 0.4402
epoch 0 iter 5450: loss = 0.4065,  smooth loss = 0.4389
epoch 0 iter 5500: loss = 0.4331,  smooth loss = 0.4371
epoch 0 iter 5550: loss = 0.4280,  smooth loss = 0.4333
epoch 0 iter 5600: loss = 0.4205,  smooth loss = 0.4327
epoch 0 iter 5650: loss = 0.4430,  smooth loss = 0.4331
epoch 0 iter 5700: loss = 0.4258,  smooth loss = 0.4312
epoch 0 iter 5750: loss = 0.4366,  smooth loss = 0.4319
epoch 0 iter 5800: loss = 0.4094,  smooth loss = 0.4322
epoch 0 iter 5850: loss = 0.4108,  smooth loss = 0.4291
epoch 0 iter 5900: loss = 0.4246,  smooth loss = 0.4248
epoch 0 iter 5950: loss = 0.3868,  smooth loss = 0.4228
epoch 0 iter 6000: loss = 0.4267,  smooth loss = 0.4213
average data time = 0.0054s, average running time = 0.5395s
epoch 0 iter 6000: eval loss = 1.8785,  ccr = 0.8342,  cwr = 0.4872,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.4872.
Save model pretrain-language-model_0_6000
epoch 0 iter 6050: loss = 0.4381,  smooth loss = 0.4217
epoch 0 iter 6100: loss = 0.4181,  smooth loss = 0.4169
epoch 0 iter 6150: loss = 0.3930,  smooth loss = 0.4158
epoch 0 iter 6200: loss = 0.4200,  smooth loss = 0.4159
epoch 0 iter 6250: loss = 0.4007,  smooth loss = 0.4155
epoch 0 iter 6300: loss = 0.3930,  smooth loss = 0.4112
epoch 0 iter 6350: loss = 0.4205,  smooth loss = 0.4120
epoch 0 iter 6400: loss = 0.4188,  smooth loss = 0.4101
epoch 0 iter 6450: loss = 0.4092,  smooth loss = 0.4104
epoch 0 iter 6500: loss = 0.4279,  smooth loss = 0.4102
epoch 0 iter 6550: loss = 0.4349,  smooth loss = 0.4068
epoch 0 iter 6600: loss = 0.4085,  smooth loss = 0.4034
epoch 0 iter 6650: loss = 0.4112,  smooth loss = 0.4044
epoch 0 iter 6700: loss = 0.4251,  smooth loss = 0.4044
epoch 0 iter 6750: loss = 0.3908,  smooth loss = 0.4017
epoch 0 iter 6800: loss = 0.4027,  smooth loss = 0.3997
epoch 0 iter 6850: loss = 0.4182,  smooth loss = 0.4018
epoch 0 iter 6900: loss = 0.4164,  smooth loss = 0.4000
epoch 0 iter 6950: loss = 0.4021,  smooth loss = 0.4011
epoch 0 iter 7000: loss = 0.4004,  smooth loss = 0.3988
Save model pretrain-language-model_0_7000
epoch 0 iter 7050: loss = 0.3918,  smooth loss = 0.3983
epoch 0 iter 7100: loss = 0.4097,  smooth loss = 0.3974
epoch 0 iter 7150: loss = 0.3905,  smooth loss = 0.3976
epoch 0 iter 7200: loss = 0.3955,  smooth loss = 0.3937
epoch 0 iter 7250: loss = 0.4087,  smooth loss = 0.3921
epoch 0 iter 7300: loss = 0.4149,  smooth loss = 0.3929
epoch 0 iter 7350: loss = 0.4250,  smooth loss = 0.3931
epoch 0 iter 7400: loss = 0.4227,  smooth loss = 0.3896
epoch 0 iter 7450: loss = 0.4035,  smooth loss = 0.3884
epoch 0 iter 7500: loss = 0.4251,  smooth loss = 0.3909
epoch 0 iter 7550: loss = 0.3945,  smooth loss = 0.3874
epoch 0 iter 7600: loss = 0.3901,  smooth loss = 0.3883
epoch 0 iter 7650: loss = 0.3647,  smooth loss = 0.3876
epoch 0 iter 7700: loss = 0.3740,  smooth loss = 0.3866
epoch 0 iter 7750: loss = 0.3860,  smooth loss = 0.3841
epoch 0 iter 7800: loss = 0.4079,  smooth loss = 0.3832
epoch 0 iter 7850: loss = 0.3417,  smooth loss = 0.3824
epoch 0 iter 7900: loss = 0.3939,  smooth loss = 0.3810
epoch 0 iter 7950: loss = 0.3723,  smooth loss = 0.3825
epoch 0 iter 8000: loss = 0.3796,  smooth loss = 0.3780
Save model pretrain-language-model_0_8000
epoch 0 iter 8050: loss = 0.3615,  smooth loss = 0.3795
epoch 0 iter 8100: loss = 0.3969,  smooth loss = 0.3792
epoch 0 iter 8150: loss = 0.3752,  smooth loss = 0.3786
epoch 0 iter 8200: loss = 0.3867,  smooth loss = 0.3759
epoch 0 iter 8250: loss = 0.3753,  smooth loss = 0.3761
epoch 0 iter 8300: loss = 0.3602,  smooth loss = 0.3737
epoch 0 iter 8350: loss = 0.3450,  smooth loss = 0.3731
epoch 0 iter 8400: loss = 0.3720,  smooth loss = 0.3720
epoch 0 iter 8450: loss = 0.3596,  smooth loss = 0.3714
epoch 0 iter 8500: loss = 0.3767,  smooth loss = 0.3714
epoch 0 iter 8550: loss = 0.3522,  smooth loss = 0.3707
epoch 0 iter 8600: loss = 0.3777,  smooth loss = 0.3697
epoch 0 iter 8650: loss = 0.3850,  smooth loss = 0.3693
epoch 0 iter 8700: loss = 0.3513,  smooth loss = 0.3680
epoch 0 iter 8750: loss = 0.3547,  smooth loss = 0.3681
epoch 0 iter 8800: loss = 0.3898,  smooth loss = 0.3695
epoch 0 iter 8850: loss = 0.3424,  smooth loss = 0.3655
epoch 0 iter 8900: loss = 0.3888,  smooth loss = 0.3670
epoch 0 iter 8950: loss = 0.3688,  smooth loss = 0.3657
epoch 0 iter 9000: loss = 0.3740,  smooth loss = 0.3676
average data time = 0.0043s, average running time = 0.5423s
epoch 0 iter 9000: eval loss = 1.9381,  ccr = 0.8354,  cwr = 0.4917,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 9000 with accuracy value: 0.4917.
Save model pretrain-language-model_0_9000
epoch 0 iter 9050: loss = 0.3636,  smooth loss = 0.3673
epoch 0 iter 9100: loss = 0.3872,  smooth loss = 0.3674
epoch 0 iter 9150: loss = 0.3268,  smooth loss = 0.3624
epoch 0 iter 9200: loss = 0.3636,  smooth loss = 0.3631
epoch 0 iter 9250: loss = 0.3465,  smooth loss = 0.3612
epoch 0 iter 9300: loss = 0.3708,  smooth loss = 0.3647
epoch 0 iter 9350: loss = 0.3531,  smooth loss = 0.3598
epoch 0 iter 9400: loss = 0.3481,  smooth loss = 0.3612
epoch 0 iter 9450: loss = 0.3592,  smooth loss = 0.3603
epoch 0 iter 9500: loss = 0.3532,  smooth loss = 0.3593
epoch 0 iter 9550: loss = 0.3647,  smooth loss = 0.3573
epoch 0 iter 9600: loss = 0.3544,  smooth loss = 0.3568
epoch 0 iter 9650: loss = 0.3547,  smooth loss = 0.3560
epoch 0 iter 9700: loss = 0.3662,  smooth loss = 0.3568
epoch 0 iter 9750: loss = 0.3471,  smooth loss = 0.3556
epoch 0 iter 9800: loss = 0.3486,  smooth loss = 0.3547
epoch 0 iter 9850: loss = 0.3484,  smooth loss = 0.3558
epoch 0 iter 9900: loss = 0.3542,  smooth loss = 0.3537
epoch 0 iter 9950: loss = 0.3461,  smooth loss = 0.3507
epoch 0 iter 10000: loss = 0.3341,  smooth loss = 0.3523
Save model pretrain-language-model_0_10000
epoch 0 iter 10050: loss = 0.3450,  smooth loss = 0.3522
epoch 0 iter 10100: loss = 0.3629,  smooth loss = 0.3521
epoch 0 iter 10150: loss = 0.3661,  smooth loss = 0.3544
epoch 0 iter 10200: loss = 0.3510,  smooth loss = 0.3528
epoch 0 iter 10250: loss = 0.3558,  smooth loss = 0.3508
epoch 0 iter 10300: loss = 0.3500,  smooth loss = 0.3505
epoch 0 iter 10350: loss = 0.3469,  smooth loss = 0.3508
epoch 0 iter 10400: loss = 0.3479,  smooth loss = 0.3501
epoch 0 iter 10450: loss = 0.3498,  smooth loss = 0.3468
epoch 0 iter 10500: loss = 0.3470,  smooth loss = 0.3458
epoch 0 iter 10550: loss = 0.3334,  smooth loss = 0.3456
epoch 0 iter 10600: loss = 0.3233,  smooth loss = 0.3438
epoch 0 iter 10650: loss = 0.3533,  smooth loss = 0.3466
epoch 0 iter 10700: loss = 0.3537,  smooth loss = 0.3457
epoch 0 iter 10750: loss = 0.3266,  smooth loss = 0.3443
epoch 0 iter 10800: loss = 0.3361,  smooth loss = 0.3434
epoch 0 iter 10850: loss = 0.3510,  smooth loss = 0.3428
epoch 0 iter 10900: loss = 0.3530,  smooth loss = 0.3439
epoch 0 iter 10950: loss = 0.3868,  smooth loss = 0.3447
epoch 0 iter 11000: loss = 0.3374,  smooth loss = 0.3431
Save model pretrain-language-model_0_11000
epoch 0 iter 11050: loss = 0.3585,  smooth loss = 0.3423
epoch 0 iter 11100: loss = 0.3352,  smooth loss = 0.3448
epoch 0 iter 11150: loss = 0.3281,  smooth loss = 0.3434
epoch 0 iter 11200: loss = 0.3536,  smooth loss = 0.3425
epoch 0 iter 11250: loss = 0.3336,  smooth loss = 0.3405
epoch 0 iter 11300: loss = 0.3489,  smooth loss = 0.3419
epoch 0 iter 11350: loss = 0.3376,  smooth loss = 0.3435
epoch 0 iter 11400: loss = 0.3328,  smooth loss = 0.3432
epoch 0 iter 11450: loss = 0.3181,  smooth loss = 0.3383
epoch 0 iter 11500: loss = 0.3507,  smooth loss = 0.3404
epoch 0 iter 11550: loss = 0.3307,  smooth loss = 0.3364
epoch 0 iter 11600: loss = 0.3523,  smooth loss = 0.3376
epoch 0 iter 11650: loss = 0.3628,  smooth loss = 0.3375
epoch 0 iter 11700: loss = 0.3429,  smooth loss = 0.3383
epoch 0 iter 11750: loss = 0.3721,  smooth loss = 0.3371
epoch 0 iter 11800: loss = 0.3263,  smooth loss = 0.3379
epoch 0 iter 11850: loss = 0.3128,  smooth loss = 0.3375
epoch 0 iter 11900: loss = 0.3509,  smooth loss = 0.3372
epoch 0 iter 11950: loss = 0.3422,  smooth loss = 0.3357
epoch 0 iter 12000: loss = 0.3303,  smooth loss = 0.3377
average data time = 0.0038s, average running time = 0.5432s
epoch 0 iter 12000: eval loss = 2.0012,  ccr = 0.8337,  cwr = 0.4896,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_12000
epoch 0 iter 12050: loss = 0.3368,  smooth loss = 0.3342
epoch 0 iter 12100: loss = 0.3065,  smooth loss = 0.3320
epoch 0 iter 12150: loss = 0.3412,  smooth loss = 0.3334
epoch 0 iter 12200: loss = 0.3370,  smooth loss = 0.3314
epoch 0 iter 12250: loss = 0.3255,  smooth loss = 0.3322
epoch 0 iter 12300: loss = 0.3213,  smooth loss = 0.3300
epoch 0 iter 12350: loss = 0.3597,  smooth loss = 0.3316
epoch 0 iter 12400: loss = 0.3384,  smooth loss = 0.3302
epoch 0 iter 12450: loss = 0.3418,  smooth loss = 0.3310
epoch 0 iter 12500: loss = 0.3200,  smooth loss = 0.3298
epoch 0 iter 12550: loss = 0.3046,  smooth loss = 0.3291
epoch 0 iter 12600: loss = 0.3271,  smooth loss = 0.3292
epoch 0 iter 12650: loss = 0.3405,  smooth loss = 0.3295
epoch 0 iter 12700: loss = 0.3384,  smooth loss = 0.3292
epoch 0 iter 12750: loss = 0.2973,  smooth loss = 0.3289
epoch 0 iter 12800: loss = 0.3157,  smooth loss = 0.3280
epoch 0 iter 12850: loss = 0.3327,  smooth loss = 0.3245
epoch 0 iter 12900: loss = 0.3515,  smooth loss = 0.3245
epoch 0 iter 12950: loss = 0.3382,  smooth loss = 0.3266
epoch 0 iter 13000: loss = 0.3280,  smooth loss = 0.3259
Save model pretrain-language-model_0_13000
epoch 0 iter 13050: loss = 0.3083,  smooth loss = 0.3291
epoch 0 iter 13100: loss = 0.3359,  smooth loss = 0.3294
epoch 0 iter 13150: loss = 0.3274,  smooth loss = 0.3275
epoch 0 iter 13200: loss = 0.3170,  smooth loss = 0.3274
epoch 0 iter 13250: loss = 0.3431,  smooth loss = 0.3260
epoch 0 iter 13300: loss = 0.3164,  smooth loss = 0.3263
epoch 0 iter 13350: loss = 0.3213,  smooth loss = 0.3249
epoch 0 iter 13400: loss = 0.3340,  smooth loss = 0.3266
epoch 0 iter 13450: loss = 0.2986,  smooth loss = 0.3254
epoch 0 iter 13500: loss = 0.3210,  smooth loss = 0.3231
epoch 0 iter 13550: loss = 0.3041,  smooth loss = 0.3228
epoch 0 iter 13600: loss = 0.3337,  smooth loss = 0.3250
epoch 0 iter 13650: loss = 0.3003,  smooth loss = 0.3222
epoch 0 iter 13700: loss = 0.3403,  smooth loss = 0.3216
epoch 0 iter 13750: loss = 0.3260,  smooth loss = 0.3215
epoch 0 iter 13800: loss = 0.3004,  smooth loss = 0.3229
epoch 0 iter 13850: loss = 0.3186,  smooth loss = 0.3205
epoch 0 iter 13900: loss = 0.3222,  smooth loss = 0.3243
epoch 0 iter 13950: loss = 0.3466,  smooth loss = 0.3218
epoch 0 iter 14000: loss = 0.3135,  smooth loss = 0.3203
Save model pretrain-language-model_0_14000
epoch 0 iter 14050: loss = 0.3374,  smooth loss = 0.3211
epoch 0 iter 14100: loss = 0.3060,  smooth loss = 0.3198
epoch 0 iter 14150: loss = 0.3150,  smooth loss = 0.3172
epoch 0 iter 14200: loss = 0.3272,  smooth loss = 0.3170
epoch 0 iter 14250: loss = 0.3360,  smooth loss = 0.3177
epoch 0 iter 14300: loss = 0.3142,  smooth loss = 0.3203
epoch 0 iter 14350: loss = 0.3098,  smooth loss = 0.3186
epoch 0 iter 14400: loss = 0.3133,  smooth loss = 0.3167
epoch 0 iter 14450: loss = 0.3177,  smooth loss = 0.3163
epoch 0 iter 14500: loss = 0.3022,  smooth loss = 0.3151
epoch 0 iter 14550: loss = 0.3389,  smooth loss = 0.3140
epoch 0 iter 14600: loss = 0.3163,  smooth loss = 0.3130
epoch 0 iter 14650: loss = 0.3223,  smooth loss = 0.3146
epoch 0 iter 14700: loss = 0.3189,  smooth loss = 0.3163
epoch 0 iter 14750: loss = 0.2901,  smooth loss = 0.3160
epoch 0 iter 14800: loss = 0.3449,  smooth loss = 0.3160
epoch 0 iter 14850: loss = 0.3263,  smooth loss = 0.3150
epoch 0 iter 14900: loss = 0.3217,  smooth loss = 0.3163
epoch 0 iter 14950: loss = 0.3297,  smooth loss = 0.3162
epoch 0 iter 15000: loss = 0.3102,  smooth loss = 0.3140
average data time = 0.0035s, average running time = 0.5441s
epoch 0 iter 15000: eval loss = 2.0358,  ccr = 0.8321,  cwr = 0.4860,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_15000
epoch 0 iter 15050: loss = 0.3110,  smooth loss = 0.3144
epoch 0 iter 15100: loss = 0.3135,  smooth loss = 0.3164
epoch 0 iter 15150: loss = 0.3042,  smooth loss = 0.3143
epoch 0 iter 15200: loss = 0.3031,  smooth loss = 0.3147
epoch 0 iter 15250: loss = 0.3126,  smooth loss = 0.3136
epoch 0 iter 15300: loss = 0.3197,  smooth loss = 0.3140
epoch 0 iter 15350: loss = 0.3018,  smooth loss = 0.3123
epoch 0 iter 15400: loss = 0.3100,  smooth loss = 0.3139
epoch 0 iter 15450: loss = 0.3024,  smooth loss = 0.3127
epoch 0 iter 15500: loss = 0.2978,  smooth loss = 0.3128
epoch 0 iter 15550: loss = 0.3203,  smooth loss = 0.3133
epoch 0 iter 15600: loss = 0.3073,  smooth loss = 0.3137
epoch 0 iter 15650: loss = 0.3019,  smooth loss = 0.3131
epoch 0 iter 15700: loss = 0.3120,  smooth loss = 0.3135
epoch 0 iter 15750: loss = 0.2974,  smooth loss = 0.3117
epoch 0 iter 15800: loss = 0.3282,  smooth loss = 0.3117
epoch 0 iter 15850: loss = 0.3090,  smooth loss = 0.3081
epoch 0 iter 15900: loss = 0.3022,  smooth loss = 0.3100
epoch 0 iter 15950: loss = 0.2968,  smooth loss = 0.3094
epoch 0 iter 16000: loss = 0.3267,  smooth loss = 0.3075
Save model pretrain-language-model_0_16000
epoch 0 iter 16050: loss = 0.2927,  smooth loss = 0.3064
epoch 0 iter 16100: loss = 0.2989,  smooth loss = 0.3065
epoch 0 iter 16150: loss = 0.3043,  smooth loss = 0.3070
epoch 0 iter 16200: loss = 0.3016,  smooth loss = 0.3076
epoch 0 iter 16250: loss = 0.3228,  smooth loss = 0.3090
epoch 0 iter 16300: loss = 0.2949,  smooth loss = 0.3095
epoch 0 iter 16350: loss = 0.2903,  smooth loss = 0.3086
epoch 0 iter 16400: loss = 0.3013,  smooth loss = 0.3081
epoch 0 iter 16450: loss = 0.3164,  smooth loss = 0.3083
epoch 0 iter 16500: loss = 0.2899,  smooth loss = 0.3045
epoch 0 iter 16550: loss = 0.3199,  smooth loss = 0.3078
epoch 0 iter 16600: loss = 0.3142,  smooth loss = 0.3080
epoch 0 iter 16650: loss = 0.3139,  smooth loss = 0.3067
epoch 0 iter 16700: loss = 0.3081,  smooth loss = 0.3064
epoch 0 iter 16750: loss = 0.2857,  smooth loss = 0.3058
epoch 0 iter 16800: loss = 0.3162,  smooth loss = 0.3057
epoch 0 iter 16850: loss = 0.3295,  smooth loss = 0.3079
epoch 0 iter 16900: loss = 0.3305,  smooth loss = 0.3057
epoch 0 iter 16950: loss = 0.3306,  smooth loss = 0.3029
epoch 0 iter 17000: loss = 0.3119,  smooth loss = 0.3056
Save model pretrain-language-model_0_17000
epoch 0 iter 17050: loss = 0.2955,  smooth loss = 0.3040
epoch 0 iter 17100: loss = 0.2835,  smooth loss = 0.3015
epoch 0 iter 17150: loss = 0.2758,  smooth loss = 0.3040
epoch 0 iter 17200: loss = 0.2841,  smooth loss = 0.3051
epoch 0 iter 17250: loss = 0.2991,  smooth loss = 0.3070
epoch 0 iter 17300: loss = 0.3178,  smooth loss = 0.3061
epoch 0 iter 17350: loss = 0.3211,  smooth loss = 0.3043
epoch 0 iter 17400: loss = 0.2941,  smooth loss = 0.3008
epoch 0 iter 17450: loss = 0.2778,  smooth loss = 0.3022
epoch 0 iter 17500: loss = 0.2909,  smooth loss = 0.3026
epoch 0 iter 17550: loss = 0.2886,  smooth loss = 0.3015
epoch 0 iter 17600: loss = 0.3117,  smooth loss = 0.3042
epoch 0 iter 17650: loss = 0.3257,  smooth loss = 0.3067
epoch 0 iter 17700: loss = 0.3296,  smooth loss = 0.3043
epoch 0 iter 17750: loss = 0.3043,  smooth loss = 0.3028
epoch 0 iter 17800: loss = 0.3261,  smooth loss = 0.3018
epoch 0 iter 17850: loss = 0.2832,  smooth loss = 0.3018
epoch 0 iter 17900: loss = 0.3000,  smooth loss = 0.3022
epoch 0 iter 17950: loss = 0.3018,  smooth loss = 0.3008
epoch 0 iter 18000: loss = 0.2879,  smooth loss = 0.3022
average data time = 0.0033s, average running time = 0.5450s
epoch 0 iter 18000: eval loss = 2.0677,  ccr = 0.8323,  cwr = 0.4880,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_18000
epoch 0 iter 18050: loss = 0.2879,  smooth loss = 0.3004
epoch 0 iter 18100: loss = 0.3159,  smooth loss = 0.2984
epoch 0 iter 18150: loss = 0.2926,  smooth loss = 0.2991
epoch 0 iter 18200: loss = 0.2847,  smooth loss = 0.3019
epoch 0 iter 18250: loss = 0.2920,  smooth loss = 0.3014
epoch 0 iter 18300: loss = 0.2896,  smooth loss = 0.3002
epoch 0 iter 18350: loss = 0.3049,  smooth loss = 0.2998
epoch 0 iter 18400: loss = 0.3185,  smooth loss = 0.2988
epoch 0 iter 18450: loss = 0.3001,  smooth loss = 0.2986
epoch 0 iter 18500: loss = 0.2981,  smooth loss = 0.2985
epoch 0 iter 18550: loss = 0.3075,  smooth loss = 0.2988
epoch 0 iter 18600: loss = 0.2959,  smooth loss = 0.2994
epoch 0 iter 18650: loss = 0.2921,  smooth loss = 0.2986
epoch 0 iter 18700: loss = 0.3056,  smooth loss = 0.2985
epoch 0 iter 18750: loss = 0.2982,  smooth loss = 0.2982
epoch 0 iter 18800: loss = 0.3095,  smooth loss = 0.2959
epoch 0 iter 18850: loss = 0.2721,  smooth loss = 0.2973
epoch 0 iter 18900: loss = 0.2886,  smooth loss = 0.2963
epoch 0 iter 18950: loss = 0.3085,  smooth loss = 0.2980
epoch 0 iter 19000: loss = 0.2983,  smooth loss = 0.2982
Save model pretrain-language-model_0_19000
epoch 0 iter 19050: loss = 0.2852,  smooth loss = 0.2991
epoch 0 iter 19100: loss = 0.2837,  smooth loss = 0.2960
epoch 0 iter 19150: loss = 0.2814,  smooth loss = 0.2959
epoch 0 iter 19200: loss = 0.2968,  smooth loss = 0.2972
epoch 0 iter 19250: loss = 0.2969,  smooth loss = 0.2963
epoch 0 iter 19300: loss = 0.2852,  smooth loss = 0.2944
epoch 0 iter 19350: loss = 0.2957,  smooth loss = 0.2951
epoch 0 iter 19400: loss = 0.2997,  smooth loss = 0.2975
epoch 0 iter 19450: loss = 0.3337,  smooth loss = 0.3001
epoch 0 iter 19500: loss = 0.2916,  smooth loss = 0.2952
epoch 0 iter 19550: loss = 0.3043,  smooth loss = 0.2958
epoch 0 iter 19600: loss = 0.3188,  smooth loss = 0.2970
epoch 0 iter 19650: loss = 0.3166,  smooth loss = 0.2952
epoch 0 iter 19700: loss = 0.2924,  smooth loss = 0.2951
epoch 0 iter 19750: loss = 0.2839,  smooth loss = 0.2944
epoch 0 iter 19800: loss = 0.2929,  smooth loss = 0.2954
epoch 0 iter 19850: loss = 0.2964,  smooth loss = 0.2946
epoch 0 iter 19900: loss = 0.2902,  smooth loss = 0.2934
epoch 0 iter 19950: loss = 0.2797,  smooth loss = 0.2952
epoch 0 iter 20000: loss = 0.2818,  smooth loss = 0.2945
Save model pretrain-language-model_0_20000
epoch 0 iter 20050: loss = 0.2718,  smooth loss = 0.2935
epoch 0 iter 20100: loss = 0.3036,  smooth loss = 0.2936
epoch 0 iter 20150: loss = 0.2942,  smooth loss = 0.2922
epoch 0 iter 20200: loss = 0.3030,  smooth loss = 0.2908
epoch 0 iter 20250: loss = 0.2767,  smooth loss = 0.2898
epoch 0 iter 20300: loss = 0.2705,  smooth loss = 0.2928
epoch 0 iter 20350: loss = 0.3129,  smooth loss = 0.2941
epoch 0 iter 20400: loss = 0.2774,  smooth loss = 0.2935
epoch 0 iter 20450: loss = 0.3095,  smooth loss = 0.2940
epoch 0 iter 20500: loss = 0.3039,  smooth loss = 0.2940
epoch 0 iter 20550: loss = 0.3027,  smooth loss = 0.2943
epoch 0 iter 20600: loss = 0.2877,  smooth loss = 0.2916
epoch 0 iter 20650: loss = 0.2881,  smooth loss = 0.2922
epoch 0 iter 20700: loss = 0.2854,  smooth loss = 0.2942
epoch 0 iter 20750: loss = 0.2962,  smooth loss = 0.2923
epoch 0 iter 20800: loss = 0.2682,  smooth loss = 0.2929
epoch 0 iter 20850: loss = 0.2766,  smooth loss = 0.2915
epoch 0 iter 20900: loss = 0.2974,  smooth loss = 0.2920
epoch 0 iter 20950: loss = 0.2894,  smooth loss = 0.2925
epoch 0 iter 21000: loss = 0.2859,  smooth loss = 0.2922
average data time = 0.0031s, average running time = 0.5452s
epoch 0 iter 21000: eval loss = 2.0878,  ccr = 0.8308,  cwr = 0.4854,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_21000
epoch 0 iter 21050: loss = 0.2785,  smooth loss = 0.2909
epoch 0 iter 21100: loss = 0.2962,  smooth loss = 0.2920
epoch 0 iter 21150: loss = 0.2897,  smooth loss = 0.2908
epoch 0 iter 21200: loss = 0.2846,  smooth loss = 0.2888
epoch 0 iter 21250: loss = 0.2906,  smooth loss = 0.2915
epoch 0 iter 21300: loss = 0.2892,  smooth loss = 0.2897
epoch 0 iter 21350: loss = 0.2816,  smooth loss = 0.2913
epoch 0 iter 21400: loss = 0.3074,  smooth loss = 0.2913
epoch 0 iter 21450: loss = 0.2890,  smooth loss = 0.2916
epoch 0 iter 21500: loss = 0.2936,  smooth loss = 0.2897
epoch 0 iter 21550: loss = 0.2850,  smooth loss = 0.2874
epoch 0 iter 21600: loss = 0.2727,  smooth loss = 0.2881
epoch 0 iter 21650: loss = 0.2927,  smooth loss = 0.2880
epoch 0 iter 21700: loss = 0.2848,  smooth loss = 0.2905
epoch 0 iter 21750: loss = 0.2719,  smooth loss = 0.2878
epoch 0 iter 21800: loss = 0.2795,  smooth loss = 0.2880
epoch 0 iter 21850: loss = 0.2837,  smooth loss = 0.2879
epoch 0 iter 21900: loss = 0.3079,  smooth loss = 0.2873
epoch 0 iter 21950: loss = 0.2716,  smooth loss = 0.2873
epoch 0 iter 22000: loss = 0.2992,  smooth loss = 0.2881
Save model pretrain-language-model_0_22000
epoch 0 iter 22050: loss = 0.2954,  smooth loss = 0.2860
epoch 0 iter 22100: loss = 0.2521,  smooth loss = 0.2859
epoch 0 iter 22150: loss = 0.2965,  smooth loss = 0.2879
epoch 0 iter 22200: loss = 0.2921,  smooth loss = 0.2890
epoch 0 iter 22250: loss = 0.2980,  smooth loss = 0.2878
epoch 0 iter 22300: loss = 0.2739,  smooth loss = 0.2887
epoch 0 iter 22350: loss = 0.2640,  smooth loss = 0.2864
epoch 0 iter 22400: loss = 0.2839,  smooth loss = 0.2850
epoch 0 iter 22450: loss = 0.3096,  smooth loss = 0.2878
epoch 0 iter 22500: loss = 0.2985,  smooth loss = 0.2874
epoch 0 iter 22550: loss = 0.2987,  smooth loss = 0.2864
epoch 0 iter 22600: loss = 0.2941,  smooth loss = 0.2845
epoch 0 iter 22650: loss = 0.2668,  smooth loss = 0.2826
epoch 0 iter 22700: loss = 0.2880,  smooth loss = 0.2853
epoch 0 iter 22750: loss = 0.2879,  smooth loss = 0.2854
epoch 0 iter 22800: loss = 0.2826,  smooth loss = 0.2851
epoch 0 iter 22850: loss = 0.2896,  smooth loss = 0.2836
epoch 0 iter 22900: loss = 0.2943,  smooth loss = 0.2856
epoch 0 iter 22950: loss = 0.2762,  smooth loss = 0.2840
epoch 0 iter 23000: loss = 0.2893,  smooth loss = 0.2849
Save model pretrain-language-model_0_23000
epoch 0 iter 23050: loss = 0.2635,  smooth loss = 0.2840
epoch 0 iter 23100: loss = 0.2868,  smooth loss = 0.2851
epoch 0 iter 23150: loss = 0.2605,  smooth loss = 0.2846
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Use 2 GPUs.
Start training.
epoch 0 iter 50: loss = 2.2598,  smooth loss = 2.4449
epoch 0 iter 100: loss = 1.9999,  smooth loss = 2.1997
epoch 0 iter 150: loss = 1.7879,  smooth loss = 1.9837
epoch 0 iter 200: loss = 1.6592,  smooth loss = 1.8024
epoch 0 iter 250: loss = 1.5684,  smooth loss = 1.6756
epoch 0 iter 300: loss = 1.5365,  smooth loss = 1.5844
epoch 0 iter 350: loss = 1.4369,  smooth loss = 1.5051
epoch 0 iter 400: loss = 1.3461,  smooth loss = 1.4405
epoch 0 iter 450: loss = 1.3287,  smooth loss = 1.3848
epoch 0 iter 500: loss = 1.3096,  smooth loss = 1.3315
epoch 0 iter 550: loss = 1.2711,  smooth loss = 1.2896
epoch 0 iter 600: loss = 1.2377,  smooth loss = 1.2474
epoch 0 iter 650: loss = 1.1635,  smooth loss = 1.2087
epoch 0 iter 700: loss = 1.1404,  smooth loss = 1.1721
epoch 0 iter 750: loss = 1.1022,  smooth loss = 1.1409
epoch 0 iter 800: loss = 1.1007,  smooth loss = 1.1111
epoch 0 iter 850: loss = 1.0399,  smooth loss = 1.0830
epoch 0 iter 900: loss = 1.0400,  smooth loss = 1.0562
epoch 0 iter 950: loss = 0.9866,  smooth loss = 1.0261
epoch 0 iter 1000: loss = 0.9890,  smooth loss = 1.0046
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9833,  smooth loss = 0.9824
epoch 0 iter 1100: loss = 0.9495,  smooth loss = 0.9606
epoch 0 iter 1150: loss = 0.9404,  smooth loss = 0.9389
epoch 0 iter 1200: loss = 0.8891,  smooth loss = 0.9156
epoch 0 iter 1250: loss = 0.8931,  smooth loss = 0.8991
epoch 0 iter 1300: loss = 0.8681,  smooth loss = 0.8826
epoch 0 iter 1350: loss = 0.8862,  smooth loss = 0.8672
epoch 0 iter 1400: loss = 0.8163,  smooth loss = 0.8478
epoch 0 iter 1450: loss = 0.7815,  smooth loss = 0.8328
epoch 0 iter 1500: loss = 0.8288,  smooth loss = 0.8179
epoch 0 iter 1550: loss = 0.7700,  smooth loss = 0.8027
epoch 0 iter 1600: loss = 0.7787,  smooth loss = 0.7915
epoch 0 iter 1650: loss = 0.7723,  smooth loss = 0.7804
epoch 0 iter 1700: loss = 0.7577,  smooth loss = 0.7651
epoch 0 iter 1750: loss = 0.7294,  smooth loss = 0.7530
epoch 0 iter 1800: loss = 0.7467,  smooth loss = 0.7388
epoch 0 iter 1850: loss = 0.7485,  smooth loss = 0.7303
epoch 0 iter 1900: loss = 0.7470,  smooth loss = 0.7217
epoch 0 iter 1950: loss = 0.7023,  smooth loss = 0.7097
epoch 0 iter 2000: loss = 0.6524,  smooth loss = 0.7007
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.6944,  smooth loss = 0.6962
epoch 0 iter 2100: loss = 0.6675,  smooth loss = 0.6857
epoch 0 iter 2150: loss = 0.6746,  smooth loss = 0.6757
epoch 0 iter 2200: loss = 0.6428,  smooth loss = 0.6650
epoch 0 iter 2250: loss = 0.6529,  smooth loss = 0.6549
epoch 0 iter 2300: loss = 0.6330,  smooth loss = 0.6494
epoch 0 iter 2350: loss = 0.6381,  smooth loss = 0.6423
epoch 0 iter 2400: loss = 0.6245,  smooth loss = 0.6356
epoch 0 iter 2450: loss = 0.6428,  smooth loss = 0.6278
epoch 0 iter 2500: loss = 0.6400,  smooth loss = 0.6242
epoch 0 iter 2550: loss = 0.6281,  smooth loss = 0.6182
epoch 0 iter 2600: loss = 0.6219,  smooth loss = 0.6146
epoch 0 iter 2650: loss = 0.6025,  smooth loss = 0.6073
epoch 0 iter 2700: loss = 0.6117,  smooth loss = 0.6010
epoch 0 iter 2750: loss = 0.6112,  smooth loss = 0.5921
epoch 0 iter 2800: loss = 0.5582,  smooth loss = 0.5888
epoch 0 iter 2850: loss = 0.5813,  smooth loss = 0.5817
epoch 0 iter 2900: loss = 0.5720,  smooth loss = 0.5784
epoch 0 iter 2950: loss = 0.5693,  smooth loss = 0.5729
epoch 0 iter 3000: loss = 0.5514,  smooth loss = 0.5677
average data time = 0.0091s, average running time = 0.5344s
epoch 0 iter 3000: eval loss = 1.8000,  ccr = 0.8376,  cwr = 0.4869,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4869.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5708,  smooth loss = 0.5653
epoch 0 iter 3100: loss = 0.5692,  smooth loss = 0.5602
epoch 0 iter 3150: loss = 0.5297,  smooth loss = 0.5552
epoch 0 iter 3200: loss = 0.5487,  smooth loss = 0.5495
epoch 0 iter 3250: loss = 0.5584,  smooth loss = 0.5479
epoch 0 iter 3300: loss = 0.5390,  smooth loss = 0.5415
epoch 0 iter 3350: loss = 0.5293,  smooth loss = 0.5397
epoch 0 iter 3400: loss = 0.5366,  smooth loss = 0.5342
epoch 0 iter 3450: loss = 0.5307,  smooth loss = 0.5329
epoch 0 iter 3500: loss = 0.5250,  smooth loss = 0.5301
epoch 0 iter 3550: loss = 0.5152,  smooth loss = 0.5293
epoch 0 iter 3600: loss = 0.5313,  smooth loss = 0.5211
epoch 0 iter 3650: loss = 0.5113,  smooth loss = 0.5168
epoch 0 iter 3700: loss = 0.5408,  smooth loss = 0.5163
epoch 0 iter 3750: loss = 0.5084,  smooth loss = 0.5118
epoch 0 iter 3800: loss = 0.4873,  smooth loss = 0.5100
epoch 0 iter 3850: loss = 0.5146,  smooth loss = 0.5066
epoch 0 iter 3900: loss = 0.5325,  smooth loss = 0.5030
epoch 0 iter 3950: loss = 0.4933,  smooth loss = 0.5013
epoch 0 iter 4000: loss = 0.4420,  smooth loss = 0.4934
Save model pretrain-language-model_0_4000
epoch 0 iter 4050: loss = 0.5122,  smooth loss = 0.4921
epoch 0 iter 4100: loss = 0.4980,  smooth loss = 0.4912
epoch 0 iter 4150: loss = 0.4876,  smooth loss = 0.4890
epoch 0 iter 4200: loss = 0.4679,  smooth loss = 0.4890
epoch 0 iter 4250: loss = 0.4821,  smooth loss = 0.4859
epoch 0 iter 4300: loss = 0.4662,  smooth loss = 0.4824
epoch 0 iter 4350: loss = 0.4819,  smooth loss = 0.4813
epoch 0 iter 4400: loss = 0.4946,  smooth loss = 0.4768
epoch 0 iter 4450: loss = 0.4785,  smooth loss = 0.4740
epoch 0 iter 4500: loss = 0.4726,  smooth loss = 0.4719
epoch 0 iter 4550: loss = 0.4812,  smooth loss = 0.4708
epoch 0 iter 4600: loss = 0.4489,  smooth loss = 0.4714
epoch 0 iter 4650: loss = 0.4578,  smooth loss = 0.4665
epoch 0 iter 4700: loss = 0.4822,  smooth loss = 0.4643
epoch 0 iter 4750: loss = 0.4673,  smooth loss = 0.4616
epoch 0 iter 4800: loss = 0.4492,  smooth loss = 0.4619
epoch 0 iter 4850: loss = 0.4285,  smooth loss = 0.4615
epoch 0 iter 4900: loss = 0.4497,  smooth loss = 0.4581
epoch 0 iter 4950: loss = 0.4470,  smooth loss = 0.4538
epoch 0 iter 5000: loss = 0.4581,  smooth loss = 0.4526
Save model pretrain-language-model_0_5000
epoch 0 iter 5050: loss = 0.4847,  smooth loss = 0.4522
epoch 0 iter 5100: loss = 0.4588,  smooth loss = 0.4481
epoch 0 iter 5150: loss = 0.4186,  smooth loss = 0.4471
epoch 0 iter 5200: loss = 0.4521,  smooth loss = 0.4480
epoch 0 iter 5250: loss = 0.4569,  smooth loss = 0.4443
epoch 0 iter 5300: loss = 0.4250,  smooth loss = 0.4437
epoch 0 iter 5350: loss = 0.4700,  smooth loss = 0.4406
epoch 0 iter 5400: loss = 0.4242,  smooth loss = 0.4398
epoch 0 iter 5450: loss = 0.4330,  smooth loss = 0.4382
epoch 0 iter 5500: loss = 0.4282,  smooth loss = 0.4355
epoch 0 iter 5550: loss = 0.4395,  smooth loss = 0.4358
epoch 0 iter 5600: loss = 0.4235,  smooth loss = 0.4342
epoch 0 iter 5650: loss = 0.4226,  smooth loss = 0.4303
epoch 0 iter 5700: loss = 0.4466,  smooth loss = 0.4265
epoch 0 iter 5750: loss = 0.4517,  smooth loss = 0.4266
epoch 0 iter 5800: loss = 0.3965,  smooth loss = 0.4243
epoch 0 iter 5850: loss = 0.4500,  smooth loss = 0.4240
epoch 0 iter 5900: loss = 0.4272,  smooth loss = 0.4205
epoch 0 iter 5950: loss = 0.4251,  smooth loss = 0.4201
epoch 0 iter 6000: loss = 0.4280,  smooth loss = 0.4223
average data time = 0.0057s, average running time = 0.5395s
epoch 0 iter 6000: eval loss = 1.8637,  ccr = 0.8365,  cwr = 0.4928,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 6000 with accuracy value: 0.4928.
Save model pretrain-language-model_0_6000
epoch 0 iter 6050: loss = 0.3998,  smooth loss = 0.4205
epoch 0 iter 6100: loss = 0.4381,  smooth loss = 0.4189
epoch 0 iter 6150: loss = 0.4197,  smooth loss = 0.4189
epoch 0 iter 6200: loss = 0.4010,  smooth loss = 0.4177
epoch 0 iter 6250: loss = 0.4109,  smooth loss = 0.4175
epoch 0 iter 6300: loss = 0.4043,  smooth loss = 0.4148
epoch 0 iter 6350: loss = 0.4007,  smooth loss = 0.4116
epoch 0 iter 6400: loss = 0.3974,  smooth loss = 0.4112
epoch 0 iter 6450: loss = 0.4233,  smooth loss = 0.4117
epoch 0 iter 6500: loss = 0.4008,  smooth loss = 0.4108
epoch 0 iter 6550: loss = 0.3839,  smooth loss = 0.4091
epoch 0 iter 6600: loss = 0.4175,  smooth loss = 0.4077
epoch 0 iter 6650: loss = 0.3864,  smooth loss = 0.4034
epoch 0 iter 6700: loss = 0.3979,  smooth loss = 0.4041
epoch 0 iter 6750: loss = 0.4171,  smooth loss = 0.4034
epoch 0 iter 6800: loss = 0.4055,  smooth loss = 0.4027
epoch 0 iter 6850: loss = 0.3693,  smooth loss = 0.3974
epoch 0 iter 6900: loss = 0.4155,  smooth loss = 0.3976
epoch 0 iter 6950: loss = 0.4089,  smooth loss = 0.3988
epoch 0 iter 7000: loss = 0.4070,  smooth loss = 0.3974
Save model pretrain-language-model_0_7000
epoch 0 iter 7050: loss = 0.4068,  smooth loss = 0.3974
epoch 0 iter 7100: loss = 0.3939,  smooth loss = 0.3969
epoch 0 iter 7150: loss = 0.3787,  smooth loss = 0.3962
epoch 0 iter 7200: loss = 0.3722,  smooth loss = 0.3919
epoch 0 iter 7250: loss = 0.4056,  smooth loss = 0.3951
epoch 0 iter 7300: loss = 0.3863,  smooth loss = 0.3906
epoch 0 iter 7350: loss = 0.3793,  smooth loss = 0.3917
epoch 0 iter 7400: loss = 0.4096,  smooth loss = 0.3904
epoch 0 iter 7450: loss = 0.3884,  smooth loss = 0.3884
epoch 0 iter 7500: loss = 0.3908,  smooth loss = 0.3878
epoch 0 iter 7550: loss = 0.4099,  smooth loss = 0.3894
epoch 0 iter 7600: loss = 0.3747,  smooth loss = 0.3871
epoch 0 iter 7650: loss = 0.3610,  smooth loss = 0.3849
epoch 0 iter 7700: loss = 0.3923,  smooth loss = 0.3852
epoch 0 iter 7750: loss = 0.3750,  smooth loss = 0.3837
epoch 0 iter 7800: loss = 0.3698,  smooth loss = 0.3841
epoch 0 iter 7850: loss = 0.3557,  smooth loss = 0.3825
epoch 0 iter 7900: loss = 0.3616,  smooth loss = 0.3835
epoch 0 iter 7950: loss = 0.3918,  smooth loss = 0.3826
epoch 0 iter 8000: loss = 0.3660,  smooth loss = 0.3813
Save model pretrain-language-model_0_8000
epoch 0 iter 8050: loss = 0.3864,  smooth loss = 0.3771
epoch 0 iter 8100: loss = 0.3781,  smooth loss = 0.3765
epoch 0 iter 8150: loss = 0.3799,  smooth loss = 0.3789
epoch 0 iter 8200: loss = 0.3786,  smooth loss = 0.3759
epoch 0 iter 8250: loss = 0.3693,  smooth loss = 0.3779
epoch 0 iter 8300: loss = 0.3792,  smooth loss = 0.3737
epoch 0 iter 8350: loss = 0.3420,  smooth loss = 0.3726
epoch 0 iter 8400: loss = 0.3744,  smooth loss = 0.3722
epoch 0 iter 8450: loss = 0.4038,  smooth loss = 0.3742
epoch 0 iter 8500: loss = 0.3640,  smooth loss = 0.3736
epoch 0 iter 8550: loss = 0.3770,  smooth loss = 0.3744
epoch 0 iter 8600: loss = 0.3538,  smooth loss = 0.3704
epoch 0 iter 8650: loss = 0.3425,  smooth loss = 0.3696
epoch 0 iter 8700: loss = 0.3639,  smooth loss = 0.3688
epoch 0 iter 8750: loss = 0.3732,  smooth loss = 0.3681
epoch 0 iter 8800: loss = 0.3676,  smooth loss = 0.3675
epoch 0 iter 8850: loss = 0.3601,  smooth loss = 0.3674
epoch 0 iter 8900: loss = 0.3615,  smooth loss = 0.3643
epoch 0 iter 8950: loss = 0.3679,  smooth loss = 0.3667
epoch 0 iter 9000: loss = 0.3432,  smooth loss = 0.3641
average data time = 0.0045s, average running time = 0.5414s
epoch 0 iter 9000: eval loss = 1.9344,  ccr = 0.8368,  cwr = 0.4932,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 9000 with accuracy value: 0.4932.
Save model pretrain-language-model_0_9000
epoch 0 iter 9050: loss = 0.3746,  smooth loss = 0.3629
epoch 0 iter 9100: loss = 0.3512,  smooth loss = 0.3627
epoch 0 iter 9150: loss = 0.3403,  smooth loss = 0.3607
epoch 0 iter 9200: loss = 0.3580,  smooth loss = 0.3625
epoch 0 iter 9250: loss = 0.3657,  smooth loss = 0.3595
epoch 0 iter 9300: loss = 0.3547,  smooth loss = 0.3612
epoch 0 iter 9350: loss = 0.3685,  smooth loss = 0.3639
epoch 0 iter 9400: loss = 0.3652,  smooth loss = 0.3597
epoch 0 iter 9450: loss = 0.3888,  smooth loss = 0.3595
epoch 0 iter 9500: loss = 0.3575,  smooth loss = 0.3595
epoch 0 iter 9550: loss = 0.3581,  smooth loss = 0.3584
epoch 0 iter 9600: loss = 0.3634,  smooth loss = 0.3563
epoch 0 iter 9650: loss = 0.3784,  smooth loss = 0.3553
epoch 0 iter 9700: loss = 0.3407,  smooth loss = 0.3554
epoch 0 iter 9750: loss = 0.3740,  smooth loss = 0.3563
epoch 0 iter 9800: loss = 0.3549,  smooth loss = 0.3562
epoch 0 iter 9850: loss = 0.3393,  smooth loss = 0.3556
epoch 0 iter 9900: loss = 0.3470,  smooth loss = 0.3553
epoch 0 iter 9950: loss = 0.3436,  smooth loss = 0.3529
epoch 0 iter 10000: loss = 0.3492,  smooth loss = 0.3513
Save model pretrain-language-model_0_10000
epoch 0 iter 10050: loss = 0.3596,  smooth loss = 0.3497
epoch 0 iter 10100: loss = 0.3717,  smooth loss = 0.3519
epoch 0 iter 10150: loss = 0.3370,  smooth loss = 0.3517
epoch 0 iter 10200: loss = 0.3297,  smooth loss = 0.3502
epoch 0 iter 10250: loss = 0.3476,  smooth loss = 0.3497
epoch 0 iter 10300: loss = 0.3552,  smooth loss = 0.3510
epoch 0 iter 10350: loss = 0.3879,  smooth loss = 0.3516
epoch 0 iter 10400: loss = 0.3454,  smooth loss = 0.3492
epoch 0 iter 10450: loss = 0.3158,  smooth loss = 0.3504
epoch 0 iter 10500: loss = 0.3720,  smooth loss = 0.3499
epoch 0 iter 10550: loss = 0.3540,  smooth loss = 0.3478
epoch 0 iter 10600: loss = 0.3324,  smooth loss = 0.3441
epoch 0 iter 10650: loss = 0.3497,  smooth loss = 0.3453
epoch 0 iter 10700: loss = 0.3279,  smooth loss = 0.3451
epoch 0 iter 10750: loss = 0.3347,  smooth loss = 0.3452
epoch 0 iter 10800: loss = 0.3121,  smooth loss = 0.3450
epoch 0 iter 10850: loss = 0.3191,  smooth loss = 0.3435
epoch 0 iter 10900: loss = 0.3581,  smooth loss = 0.3425
epoch 0 iter 10950: loss = 0.3574,  smooth loss = 0.3460
epoch 0 iter 11000: loss = 0.3478,  smooth loss = 0.3417
Save model pretrain-language-model_0_11000
epoch 0 iter 11050: loss = 0.3679,  smooth loss = 0.3417
epoch 0 iter 11100: loss = 0.3374,  smooth loss = 0.3432
epoch 0 iter 11150: loss = 0.3350,  smooth loss = 0.3429
epoch 0 iter 11200: loss = 0.3352,  smooth loss = 0.3429
epoch 0 iter 11250: loss = 0.3431,  smooth loss = 0.3411
epoch 0 iter 11300: loss = 0.3413,  smooth loss = 0.3403
epoch 0 iter 11350: loss = 0.3213,  smooth loss = 0.3381
epoch 0 iter 11400: loss = 0.3089,  smooth loss = 0.3378
epoch 0 iter 11450: loss = 0.3417,  smooth loss = 0.3372
epoch 0 iter 11500: loss = 0.3404,  smooth loss = 0.3376
epoch 0 iter 11550: loss = 0.3303,  smooth loss = 0.3353
epoch 0 iter 11600: loss = 0.3341,  smooth loss = 0.3346
epoch 0 iter 11650: loss = 0.3423,  smooth loss = 0.3351
epoch 0 iter 11700: loss = 0.3388,  smooth loss = 0.3344
epoch 0 iter 11750: loss = 0.3431,  smooth loss = 0.3360
epoch 0 iter 11800: loss = 0.3382,  smooth loss = 0.3363
epoch 0 iter 11850: loss = 0.3428,  smooth loss = 0.3357
epoch 0 iter 11900: loss = 0.3328,  smooth loss = 0.3357
epoch 0 iter 11950: loss = 0.3390,  smooth loss = 0.3338
epoch 0 iter 12000: loss = 0.3192,  smooth loss = 0.3313
average data time = 0.0040s, average running time = 0.5423s
epoch 0 iter 12000: eval loss = 2.0030,  ccr = 0.8369,  cwr = 0.4907,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_12000
epoch 0 iter 12050: loss = 0.3326,  smooth loss = 0.3328
epoch 0 iter 12100: loss = 0.3277,  smooth loss = 0.3325
epoch 0 iter 12150: loss = 0.3245,  smooth loss = 0.3327
epoch 0 iter 12200: loss = 0.3035,  smooth loss = 0.3314
epoch 0 iter 12250: loss = 0.3375,  smooth loss = 0.3309
epoch 0 iter 12300: loss = 0.3361,  smooth loss = 0.3300
epoch 0 iter 12350: loss = 0.3407,  smooth loss = 0.3306
epoch 0 iter 12400: loss = 0.3207,  smooth loss = 0.3317
epoch 0 iter 12450: loss = 0.3069,  smooth loss = 0.3322
epoch 0 iter 12500: loss = 0.3329,  smooth loss = 0.3288
epoch 0 iter 12550: loss = 0.3547,  smooth loss = 0.3292
epoch 0 iter 12600: loss = 0.3353,  smooth loss = 0.3287
epoch 0 iter 12650: loss = 0.3228,  smooth loss = 0.3274
epoch 0 iter 12700: loss = 0.3148,  smooth loss = 0.3272
epoch 0 iter 12750: loss = 0.3206,  smooth loss = 0.3268
epoch 0 iter 12800: loss = 0.3269,  smooth loss = 0.3273
epoch 0 iter 12850: loss = 0.3048,  smooth loss = 0.3280
epoch 0 iter 12900: loss = 0.3137,  smooth loss = 0.3276
epoch 0 iter 12950: loss = 0.3122,  smooth loss = 0.3249
epoch 0 iter 13000: loss = 0.3350,  smooth loss = 0.3268
Save model pretrain-language-model_0_13000
epoch 0 iter 13050: loss = 0.3172,  smooth loss = 0.3283
epoch 0 iter 13100: loss = 0.3076,  smooth loss = 0.3259
epoch 0 iter 13150: loss = 0.3160,  smooth loss = 0.3274
epoch 0 iter 13200: loss = 0.3232,  smooth loss = 0.3230
epoch 0 iter 13250: loss = 0.3302,  smooth loss = 0.3222
epoch 0 iter 13300: loss = 0.3398,  smooth loss = 0.3243
epoch 0 iter 13350: loss = 0.3115,  smooth loss = 0.3250
epoch 0 iter 13400: loss = 0.3504,  smooth loss = 0.3257
epoch 0 iter 13450: loss = 0.3240,  smooth loss = 0.3235
epoch 0 iter 13500: loss = 0.3325,  smooth loss = 0.3232
epoch 0 iter 13550: loss = 0.3011,  smooth loss = 0.3223
epoch 0 iter 13600: loss = 0.3259,  smooth loss = 0.3230
epoch 0 iter 13650: loss = 0.2907,  smooth loss = 0.3217
epoch 0 iter 13700: loss = 0.3300,  smooth loss = 0.3240
epoch 0 iter 13750: loss = 0.3243,  smooth loss = 0.3230
epoch 0 iter 13800: loss = 0.3156,  smooth loss = 0.3228
epoch 0 iter 13850: loss = 0.3317,  smooth loss = 0.3211
epoch 0 iter 13900: loss = 0.2981,  smooth loss = 0.3216
epoch 0 iter 13950: loss = 0.3159,  smooth loss = 0.3216
epoch 0 iter 14000: loss = 0.3208,  smooth loss = 0.3226
Save model pretrain-language-model_0_14000
epoch 0 iter 14050: loss = 0.3203,  smooth loss = 0.3205
epoch 0 iter 14100: loss = 0.3190,  smooth loss = 0.3200
epoch 0 iter 14150: loss = 0.3440,  smooth loss = 0.3184
epoch 0 iter 14200: loss = 0.3197,  smooth loss = 0.3199
epoch 0 iter 14250: loss = 0.3254,  smooth loss = 0.3173
epoch 0 iter 14300: loss = 0.3072,  smooth loss = 0.3184
epoch 0 iter 14350: loss = 0.3093,  smooth loss = 0.3175
epoch 0 iter 14400: loss = 0.3252,  smooth loss = 0.3158
epoch 0 iter 14450: loss = 0.3214,  smooth loss = 0.3179
epoch 0 iter 14500: loss = 0.3016,  smooth loss = 0.3179
epoch 0 iter 14550: loss = 0.3038,  smooth loss = 0.3170
epoch 0 iter 14600: loss = 0.3168,  smooth loss = 0.3185
epoch 0 iter 14650: loss = 0.3135,  smooth loss = 0.3161
epoch 0 iter 14700: loss = 0.2831,  smooth loss = 0.3170
epoch 0 iter 14750: loss = 0.3037,  smooth loss = 0.3145
epoch 0 iter 14800: loss = 0.3249,  smooth loss = 0.3154
epoch 0 iter 14850: loss = 0.2920,  smooth loss = 0.3133
epoch 0 iter 14900: loss = 0.3023,  smooth loss = 0.3140
epoch 0 iter 14950: loss = 0.3305,  smooth loss = 0.3152
epoch 0 iter 15000: loss = 0.2980,  smooth loss = 0.3132
average data time = 0.0037s, average running time = 0.5426s
epoch 0 iter 15000: eval loss = 2.0219,  ccr = 0.8382,  cwr = 0.4954,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 15000 with accuracy value: 0.4954.
Save model pretrain-language-model_0_15000
epoch 0 iter 15050: loss = 0.3152,  smooth loss = 0.3137
epoch 0 iter 15100: loss = 0.3054,  smooth loss = 0.3144
epoch 0 iter 15150: loss = 0.3169,  smooth loss = 0.3137
epoch 0 iter 15200: loss = 0.3164,  smooth loss = 0.3137
epoch 0 iter 15250: loss = 0.3171,  smooth loss = 0.3119
epoch 0 iter 15300: loss = 0.3091,  smooth loss = 0.3120
epoch 0 iter 15350: loss = 0.3004,  smooth loss = 0.3122
epoch 0 iter 15400: loss = 0.3136,  smooth loss = 0.3112
epoch 0 iter 15450: loss = 0.2865,  smooth loss = 0.3092
epoch 0 iter 15500: loss = 0.3003,  smooth loss = 0.3100
epoch 0 iter 15550: loss = 0.3114,  smooth loss = 0.3101
epoch 0 iter 15600: loss = 0.3236,  smooth loss = 0.3128
epoch 0 iter 15650: loss = 0.3103,  smooth loss = 0.3126
epoch 0 iter 15700: loss = 0.2911,  smooth loss = 0.3107
epoch 0 iter 15750: loss = 0.3038,  smooth loss = 0.3097
epoch 0 iter 15800: loss = 0.3073,  smooth loss = 0.3088
epoch 0 iter 15850: loss = 0.3060,  smooth loss = 0.3093
epoch 0 iter 15900: loss = 0.3199,  smooth loss = 0.3112
epoch 0 iter 15950: loss = 0.2961,  smooth loss = 0.3083
epoch 0 iter 16000: loss = 0.2946,  smooth loss = 0.3112
Save model pretrain-language-model_0_16000
epoch 0 iter 16050: loss = 0.3050,  smooth loss = 0.3100
epoch 0 iter 16100: loss = 0.3231,  smooth loss = 0.3097
epoch 0 iter 16150: loss = 0.2944,  smooth loss = 0.3078
epoch 0 iter 16200: loss = 0.2953,  smooth loss = 0.3078
epoch 0 iter 16250: loss = 0.3013,  smooth loss = 0.3076
epoch 0 iter 16300: loss = 0.2792,  smooth loss = 0.3072
epoch 0 iter 16350: loss = 0.3011,  smooth loss = 0.3059
epoch 0 iter 16400: loss = 0.3339,  smooth loss = 0.3067
epoch 0 iter 16450: loss = 0.3035,  smooth loss = 0.3075
epoch 0 iter 16500: loss = 0.3213,  smooth loss = 0.3070
epoch 0 iter 16550: loss = 0.2811,  smooth loss = 0.3064
epoch 0 iter 16600: loss = 0.2911,  smooth loss = 0.3047
epoch 0 iter 16650: loss = 0.2883,  smooth loss = 0.3050
epoch 0 iter 16700: loss = 0.2748,  smooth loss = 0.3039
epoch 0 iter 16750: loss = 0.3131,  smooth loss = 0.3037
epoch 0 iter 16800: loss = 0.3079,  smooth loss = 0.3061
epoch 0 iter 16850: loss = 0.3036,  smooth loss = 0.3051
epoch 0 iter 16900: loss = 0.3121,  smooth loss = 0.3026
epoch 0 iter 16950: loss = 0.2662,  smooth loss = 0.3012
epoch 0 iter 17000: loss = 0.2996,  smooth loss = 0.3015
Save model pretrain-language-model_0_17000
epoch 0 iter 17050: loss = 0.3000,  smooth loss = 0.3021
epoch 0 iter 17100: loss = 0.2948,  smooth loss = 0.3021
epoch 0 iter 17150: loss = 0.2834,  smooth loss = 0.3023
epoch 0 iter 17200: loss = 0.3112,  smooth loss = 0.3024
epoch 0 iter 17250: loss = 0.2946,  smooth loss = 0.3037
epoch 0 iter 17300: loss = 0.3104,  smooth loss = 0.3025
epoch 0 iter 17350: loss = 0.3248,  smooth loss = 0.3032
epoch 0 iter 17400: loss = 0.3098,  smooth loss = 0.3026
epoch 0 iter 17450: loss = 0.2987,  smooth loss = 0.3032
epoch 0 iter 17500: loss = 0.3335,  smooth loss = 0.3012
epoch 0 iter 17550: loss = 0.2992,  smooth loss = 0.3029
epoch 0 iter 17600: loss = 0.3113,  smooth loss = 0.3024
epoch 0 iter 17650: loss = 0.2995,  smooth loss = 0.3033
epoch 0 iter 17700: loss = 0.2807,  smooth loss = 0.3007
epoch 0 iter 17750: loss = 0.2896,  smooth loss = 0.3009
epoch 0 iter 17800: loss = 0.2856,  smooth loss = 0.3002
epoch 0 iter 17850: loss = 0.2939,  smooth loss = 0.3006
epoch 0 iter 17900: loss = 0.2893,  smooth loss = 0.2985
epoch 0 iter 17950: loss = 0.3171,  smooth loss = 0.3015
epoch 0 iter 18000: loss = 0.2869,  smooth loss = 0.3014
average data time = 0.0034s, average running time = 0.5433s
epoch 0 iter 18000: eval loss = 2.0793,  ccr = 0.8384,  cwr = 0.4966,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 18000 with accuracy value: 0.4966.
Save model pretrain-language-model_0_18000
epoch 0 iter 18050: loss = 0.2996,  smooth loss = 0.3008
epoch 0 iter 18100: loss = 0.3048,  smooth loss = 0.3024
epoch 0 iter 18150: loss = 0.3144,  smooth loss = 0.3009
epoch 0 iter 18200: loss = 0.2995,  smooth loss = 0.3002
epoch 0 iter 18250: loss = 0.3264,  smooth loss = 0.3010
epoch 0 iter 18300: loss = 0.2989,  smooth loss = 0.2999
epoch 0 iter 18350: loss = 0.2872,  smooth loss = 0.2997
epoch 0 iter 18400: loss = 0.3175,  smooth loss = 0.2998
epoch 0 iter 18450: loss = 0.2834,  smooth loss = 0.2982
epoch 0 iter 18500: loss = 0.3085,  smooth loss = 0.2969
epoch 0 iter 18550: loss = 0.3192,  smooth loss = 0.2993
epoch 0 iter 18600: loss = 0.3053,  smooth loss = 0.2965
epoch 0 iter 18650: loss = 0.3047,  smooth loss = 0.2997
epoch 0 iter 18700: loss = 0.3051,  smooth loss = 0.2966
epoch 0 iter 18750: loss = 0.3176,  smooth loss = 0.2976
epoch 0 iter 18800: loss = 0.3050,  smooth loss = 0.2964
epoch 0 iter 18850: loss = 0.2799,  smooth loss = 0.2969
epoch 0 iter 18900: loss = 0.2992,  smooth loss = 0.2965
epoch 0 iter 18950: loss = 0.3167,  smooth loss = 0.2984
epoch 0 iter 19000: loss = 0.3125,  smooth loss = 0.2981
Save model pretrain-language-model_0_19000
epoch 0 iter 19050: loss = 0.2857,  smooth loss = 0.2973
epoch 0 iter 19100: loss = 0.2758,  smooth loss = 0.2940
epoch 0 iter 19150: loss = 0.3089,  smooth loss = 0.2959
epoch 0 iter 19200: loss = 0.2577,  smooth loss = 0.2947
epoch 0 iter 19250: loss = 0.2997,  smooth loss = 0.2960
epoch 0 iter 19300: loss = 0.3028,  smooth loss = 0.2955
epoch 0 iter 19350: loss = 0.2818,  smooth loss = 0.2942
epoch 0 iter 19400: loss = 0.2906,  smooth loss = 0.2935
epoch 0 iter 19450: loss = 0.2840,  smooth loss = 0.2930
epoch 0 iter 19500: loss = 0.2960,  smooth loss = 0.2932
epoch 0 iter 19550: loss = 0.2993,  smooth loss = 0.2932
epoch 0 iter 19600: loss = 0.2954,  smooth loss = 0.2937
epoch 0 iter 19650: loss = 0.2655,  smooth loss = 0.2945
epoch 0 iter 19700: loss = 0.3066,  smooth loss = 0.2967
epoch 0 iter 19750: loss = 0.2920,  smooth loss = 0.2952
epoch 0 iter 19800: loss = 0.2968,  smooth loss = 0.2946
epoch 0 iter 19850: loss = 0.2946,  smooth loss = 0.2958
epoch 0 iter 19900: loss = 0.2908,  smooth loss = 0.2949
epoch 0 iter 19950: loss = 0.2922,  smooth loss = 0.2945
epoch 0 iter 20000: loss = 0.2852,  smooth loss = 0.2942
Save model pretrain-language-model_0_20000
epoch 0 iter 20050: loss = 0.2715,  smooth loss = 0.2937
epoch 0 iter 20100: loss = 0.2867,  smooth loss = 0.2913
epoch 0 iter 20150: loss = 0.2833,  smooth loss = 0.2937
epoch 0 iter 20200: loss = 0.2620,  smooth loss = 0.2929
epoch 0 iter 20250: loss = 0.2867,  smooth loss = 0.2916
epoch 0 iter 20300: loss = 0.2986,  smooth loss = 0.2916
epoch 0 iter 20350: loss = 0.3276,  smooth loss = 0.2925
epoch 0 iter 20400: loss = 0.2885,  smooth loss = 0.2924
epoch 0 iter 20450: loss = 0.3014,  smooth loss = 0.2926
epoch 0 iter 20500: loss = 0.2710,  smooth loss = 0.2911
epoch 0 iter 20550: loss = 0.2831,  smooth loss = 0.2898
epoch 0 iter 20600: loss = 0.2765,  smooth loss = 0.2893
epoch 0 iter 20650: loss = 0.2982,  smooth loss = 0.2896
epoch 0 iter 20700: loss = 0.2787,  smooth loss = 0.2893
epoch 0 iter 20750: loss = 0.2916,  smooth loss = 0.2907
epoch 0 iter 20800: loss = 0.2900,  smooth loss = 0.2914
epoch 0 iter 20850: loss = 0.2724,  smooth loss = 0.2902
epoch 0 iter 20900: loss = 0.2693,  smooth loss = 0.2898
epoch 0 iter 20950: loss = 0.3158,  smooth loss = 0.2911
epoch 0 iter 21000: loss = 0.2790,  smooth loss = 0.2894
average data time = 0.0033s, average running time = 0.5436s
epoch 0 iter 21000: eval loss = 2.1013,  ccr = 0.8383,  cwr = 0.4946,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Save model pretrain-language-model_0_21000
epoch 0 iter 21050: loss = 0.2729,  smooth loss = 0.2889
epoch 0 iter 21100: loss = 0.2836,  smooth loss = 0.2908
epoch 0 iter 21150: loss = 0.2947,  smooth loss = 0.2898
epoch 0 iter 21200: loss = 0.2900,  smooth loss = 0.2898
epoch 0 iter 21250: loss = 0.3055,  smooth loss = 0.2892
epoch 0 iter 21300: loss = 0.2725,  smooth loss = 0.2898
epoch 0 iter 21350: loss = 0.2858,  smooth loss = 0.2898
epoch 0 iter 21400: loss = 0.3008,  smooth loss = 0.2914
epoch 0 iter 21450: loss = 0.2717,  smooth loss = 0.2882
epoch 0 iter 21500: loss = 0.3086,  smooth loss = 0.2900
epoch 0 iter 21550: loss = 0.2784,  smooth loss = 0.2866
epoch 0 iter 21600: loss = 0.2804,  smooth loss = 0.2881
epoch 0 iter 21650: loss = 0.2875,  smooth loss = 0.2880
epoch 0 iter 21700: loss = 0.2822,  smooth loss = 0.2867
epoch 0 iter 21750: loss = 0.2856,  smooth loss = 0.2858
epoch 0 iter 21800: loss = 0.2894,  smooth loss = 0.2882
epoch 0 iter 21850: loss = 0.3151,  smooth loss = 0.2894
epoch 0 iter 21900: loss = 0.2972,  smooth loss = 0.2872
epoch 0 iter 21950: loss = 0.2812,  smooth loss = 0.2875
epoch 0 iter 22000: loss = 0.2696,  smooth loss = 0.2881
Save model pretrain-language-model_0_22000
epoch 0 iter 22050: loss = 0.2912,  smooth loss = 0.2869
epoch 0 iter 22100: loss = 0.2495,  smooth loss = 0.2866
epoch 0 iter 22150: loss = 0.2963,  smooth loss = 0.2861
epoch 0 iter 22200: loss = 0.2782,  smooth loss = 0.2892
epoch 0 iter 22250: loss = 0.2977,  smooth loss = 0.2879
epoch 0 iter 22300: loss = 0.2915,  smooth loss = 0.2856
epoch 0 iter 22350: loss = 0.2842,  smooth loss = 0.2861
epoch 0 iter 22400: loss = 0.3080,  smooth loss = 0.2855
epoch 0 iter 22450: loss = 0.2823,  smooth loss = 0.2872
epoch 0 iter 22500: loss = 0.2918,  smooth loss = 0.2853
epoch 0 iter 22550: loss = 0.2808,  smooth loss = 0.2861
epoch 0 iter 22600: loss = 0.2958,  smooth loss = 0.2866
epoch 0 iter 22650: loss = 0.2755,  smooth loss = 0.2849
epoch 0 iter 22700: loss = 0.2831,  smooth loss = 0.2840
epoch 0 iter 22750: loss = 0.2685,  smooth loss = 0.2841
epoch 0 iter 22800: loss = 0.2755,  smooth loss = 0.2857
epoch 0 iter 22850: loss = 0.2568,  smooth loss = 0.2832
epoch 0 iter 22900: loss = 0.2666,  smooth loss = 0.2845
epoch 0 iter 22950: loss = 0.3031,  smooth loss = 0.2825
epoch 0 iter 23000: loss = 0.2722,  smooth loss = 0.2834
Save model pretrain-language-model_0_23000
epoch 0 iter 23050: loss = 0.2983,  smooth loss = 0.2835
epoch 0 iter 23100: loss = 0.3045,  smooth loss = 0.2845
epoch 0 iter 23150: loss = 0.2926,  smooth loss = 0.2834
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Use 2 GPUs.
Start training.
epoch 0 iter 50: loss = 2.2473,  smooth loss = 2.4303
epoch 0 iter 100: loss = 2.0120,  smooth loss = 2.1765
epoch 0 iter 150: loss = 1.7528,  smooth loss = 1.9512
epoch 0 iter 200: loss = 1.6665,  smooth loss = 1.7817
epoch 0 iter 250: loss = 1.5523,  smooth loss = 1.6668
epoch 0 iter 300: loss = 1.4879,  smooth loss = 1.5788
epoch 0 iter 350: loss = 1.4365,  smooth loss = 1.5072
epoch 0 iter 400: loss = 1.3887,  smooth loss = 1.4426
epoch 0 iter 450: loss = 1.3355,  smooth loss = 1.3911
epoch 0 iter 500: loss = 1.2812,  smooth loss = 1.3407
epoch 0 iter 550: loss = 1.2619,  smooth loss = 1.2944
epoch 0 iter 600: loss = 1.2123,  smooth loss = 1.2533
epoch 0 iter 650: loss = 1.1629,  smooth loss = 1.2164
epoch 0 iter 700: loss = 1.1780,  smooth loss = 1.1800
epoch 0 iter 750: loss = 1.1134,  smooth loss = 1.1468
epoch 0 iter 800: loss = 1.0728,  smooth loss = 1.1151
epoch 0 iter 850: loss = 1.0405,  smooth loss = 1.0804
epoch 0 iter 900: loss = 1.0083,  smooth loss = 1.0521
epoch 0 iter 950: loss = 0.9981,  smooth loss = 1.0301
epoch 0 iter 1000: loss = 0.9743,  smooth loss = 1.0076
Save model pretrain-language-model_0_1000
epoch 0 iter 1050: loss = 0.9427,  smooth loss = 0.9832
epoch 0 iter 1100: loss = 0.9427,  smooth loss = 0.9577
epoch 0 iter 1150: loss = 0.9068,  smooth loss = 0.9363
epoch 0 iter 1200: loss = 0.8957,  smooth loss = 0.9160
epoch 0 iter 1250: loss = 0.9059,  smooth loss = 0.8978
epoch 0 iter 1300: loss = 0.8508,  smooth loss = 0.8802
epoch 0 iter 1350: loss = 0.8712,  smooth loss = 0.8625
epoch 0 iter 1400: loss = 0.8402,  smooth loss = 0.8477
epoch 0 iter 1450: loss = 0.8048,  smooth loss = 0.8300
epoch 0 iter 1500: loss = 0.8193,  smooth loss = 0.8172
epoch 0 iter 1550: loss = 0.7881,  smooth loss = 0.8006
epoch 0 iter 1600: loss = 0.8139,  smooth loss = 0.7892
epoch 0 iter 1650: loss = 0.7323,  smooth loss = 0.7744
epoch 0 iter 1700: loss = 0.7490,  smooth loss = 0.7616
epoch 0 iter 1750: loss = 0.7518,  smooth loss = 0.7515
epoch 0 iter 1800: loss = 0.7167,  smooth loss = 0.7430
epoch 0 iter 1850: loss = 0.7122,  smooth loss = 0.7290
epoch 0 iter 1900: loss = 0.7080,  smooth loss = 0.7166
epoch 0 iter 1950: loss = 0.6958,  smooth loss = 0.7115
epoch 0 iter 2000: loss = 0.7063,  smooth loss = 0.7008
Save model pretrain-language-model_0_2000
epoch 0 iter 2050: loss = 0.7081,  smooth loss = 0.6914
epoch 0 iter 2100: loss = 0.6504,  smooth loss = 0.6818
epoch 0 iter 2150: loss = 0.6660,  smooth loss = 0.6727
epoch 0 iter 2200: loss = 0.6605,  smooth loss = 0.6659
epoch 0 iter 2250: loss = 0.6365,  smooth loss = 0.6579
epoch 0 iter 2300: loss = 0.6632,  smooth loss = 0.6508
epoch 0 iter 2350: loss = 0.6232,  smooth loss = 0.6419
epoch 0 iter 2400: loss = 0.6371,  smooth loss = 0.6375
epoch 0 iter 2450: loss = 0.6072,  smooth loss = 0.6286
epoch 0 iter 2500: loss = 0.5959,  smooth loss = 0.6214
epoch 0 iter 2550: loss = 0.5933,  smooth loss = 0.6189
epoch 0 iter 2600: loss = 0.6667,  smooth loss = 0.6119
epoch 0 iter 2650: loss = 0.6049,  smooth loss = 0.6038
epoch 0 iter 2700: loss = 0.5917,  smooth loss = 0.5962
epoch 0 iter 2750: loss = 0.6052,  smooth loss = 0.5940
epoch 0 iter 2800: loss = 0.5615,  smooth loss = 0.5869
epoch 0 iter 2850: loss = 0.5622,  smooth loss = 0.5816
epoch 0 iter 2900: loss = 0.5785,  smooth loss = 0.5733
epoch 0 iter 2950: loss = 0.5578,  smooth loss = 0.5709
epoch 0 iter 3000: loss = 0.5437,  smooth loss = 0.5657
average data time = 0.0093s, average running time = 0.5342s
epoch 0 iter 3000: eval loss = 1.7988,  ccr = 0.8384,  cwr = 0.4921,  ted = 0.0000,  ned = 0.0000,  ted/w = 0.0000, 
Better model found at epoch 0, iter 3000 with accuracy value: 0.4921.
Save model pretrain-language-model_0_3000
epoch 0 iter 3050: loss = 0.5711,  smooth loss = 0.5636
epoch 0 iter 3100: loss = 0.5672,  smooth loss = 0.5591
epoch 0 iter 3150: loss = 0.5647,  smooth loss = 0.5563
epoch 0 iter 3200: loss = 0.5327,  smooth loss = 0.5511
epoch 0 iter 3250: loss = 0.5286,  smooth loss = 0.5457
epoch 0 iter 3300: loss = 0.5555,  smooth loss = 0.5434
epoch 0 iter 3350: loss = 0.5242,  smooth loss = 0.5401
epoch 0 iter 3400: loss = 0.5518,  smooth loss = 0.5371
epoch 0 iter 3450: loss = 0.5442,  smooth loss = 0.5320
epoch 0 iter 3500: loss = 0.5158,  smooth loss = 0.5250
epoch 0 iter 3550: loss = 0.5125,  smooth loss = 0.5226
epoch 0 iter 3600: loss = 0.5203,  smooth loss = 0.5214
epoch 0 iter 3650: loss = 0.5096,  smooth loss = 0.5178
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2558,  smooth loss = 2.4584
epoch 0 iter 100: loss = 1.9732,  smooth loss = 2.1897
epoch 0 iter 150: loss = 1.7884,  smooth loss = 1.9635
epoch 0 iter 200: loss = 1.6468,  smooth loss = 1.7933
epoch 0 iter 250: loss = 1.5794,  smooth loss = 1.6694
epoch 0 iter 300: loss = 1.4970,  smooth loss = 1.5788
epoch 0 iter 350: loss = 1.4447,  smooth loss = 1.5021
epoch 0 iter 400: loss = 1.3818,  smooth loss = 1.4358
epoch 0 iter 450: loss = 1.3204,  smooth loss = 1.3752
epoch 0 iter 500: loss = 1.2784,  smooth loss = 1.3262
epoch 0 iter 550: loss = 1.2356,  smooth loss = 1.2793
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2487,  smooth loss = 2.4493
epoch 0 iter 100: loss = 1.9866,  smooth loss = 2.1927
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 1
	(39): training_eval_iters = 3000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2791,  smooth loss = 2.4741
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositiModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2623,  smooth loss = 2.4505
epoch 0 iter 100: loss = 1.9845,  smooth loss = 2.1914
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2350,  smooth loss = 2.4372
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2621,  smooth loss = 2.4492
epoch 0 iter 100: loss = 2.0416,  smooth loss = 2.2022
epoch 0 iter 150: loss = 1.7787,  smooth loss = 1.9790
epoch 0 iter 200: loss = 1.6532,  smooth loss = 1.8080
epoch 0 iter 250: loss = 1.6027,  smooth loss = 1.6793
epoch 0 iter 300: loss = 1.5149,  smooth loss = 1.5871
epoch 0 iter 350: loss = 1.4656,  smooth loss = 1.5106
epoch 0 iter 400: loss = 1.3980,  smooth loss = 1.4461
epoch 0 iter 450: loss = 1.3173,  smooth loss = 1.3911
epoch 0 iter 500: loss = 1.2726,  smooth loss = 1.3408
epoch 0 iter 550: loss = 1.2202,  smooth loss = 1.2907
epoch 0 iter 600: loss = 1.2169,  smooth loss = 1.2472
epoch 0 iter 650: loss = 1.1693,  smooth loss = 1.2101
epoch 0 iter 700: loss = 1.1546,  smooth loss = 1.1755
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = /home2/tanisha/ABINetNEW/data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=37, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=37, bias=True)
)
Construct learner.
Start training.
epoch 0 iter 50: loss = 2.2444,  smooth loss = 2.4550
epoch 0 iter 100: loss = 1.9829,  smooth loss = 2.1971
epoch 0 iter 150: loss = 1.7707,  smooth loss = 1.9681
epoch 0 iter 200: loss = 1.6563,  smooth loss = 1.7945
epoch 0 iter 250: loss = 1.5984,  smooth loss = 1.6777
epoch 0 iter 300: loss = 1.4912,  smooth loss = 1.5850
epoch 0 iter 350: loss = 1.4747,  smooth loss = 1.5087
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/WikiText-103_eval_d1.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/WikiText-103.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
86460763 training items found.
50000 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=183, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=183, bias=True)
)
Construct learner.
Start training.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_bl.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 183
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 2048
	(14): dataset_test_roots = ['data/wiki_bg_text_eval.csv']
	(15): dataset_train_batch_size = 2048
	(16): dataset_train_roots = ['data/wiki_bg_text.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 10
	(39): training_eval_iters = 6000
	(40): training_save_iters = 1000
	(41): training_show_iters = 50
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
722712 training items found.
64982 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=55, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=55, bias=True)
)
Construct learner.
Start training.
